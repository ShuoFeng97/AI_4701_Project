{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e416ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d88c37f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 58) (2000,)\n",
      "(1340, 58) (660, 58) (1340,) (660,)\n"
     ]
    }
   ],
   "source": [
    "# df_gemaps=gemaps\n",
    "# df_gemaps['filename']=fname\n",
    "df=pd.read_csv('./mfcc/features_3_sec.csv')\n",
    "# df=df_gemaps.merge(df,on='filename')\n",
    "\n",
    "\n",
    "# cnn=pd.read_csv(\"CNN_featrues.csv\")\n",
    "# df=pd.concat([cnn,df],axis=1)\n",
    "df=df.drop(labels=\"filename\",axis=1)\n",
    "df=df[(df.label=='blues')|(df.label=='jazz')]\n",
    "# df=df[['rms_var', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'mfcc5_var', 'mfcc6_mean', 'mfcc7_var', 'mfcc10_var', 'mfcc17_mean', 'mfcc20_var','label']]\n",
    "df=df.sample(n=2000)\n",
    "df.label.value_counts()\n",
    "X=StandardScaler().fit_transform(np.array(df.iloc[:,:-1],dtype=float))\n",
    "class_list=df.iloc[:,-1]\n",
    "converter=LabelEncoder()\n",
    "y=converter.fit_transform(class_list)\n",
    "print(X.shape,y.shape)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33)\n",
    "# print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "X_train=X[0:1340,:]\n",
    "y_train=y[0:1340]\n",
    "X_test=X[1340:,:]\n",
    "y_test=y[1340:]\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "mfccs_total=X\n",
    "labels_total=y\n",
    "labels_train=y_train\n",
    "labels_test=y_test\n",
    "\n",
    "kpca_mfccs_total=mfccs_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4500410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1128.42it/s]\n",
      "100%|██████████| 2000/2000 [00:02<00:00, 851.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2000, num_edges=0,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "num_neigh = 800\n",
    "neigh1 = NearestNeighbors(n_neighbors=num_neigh, radius=1,metric='jaccard')\n",
    "neigh1.fit(kpca_mfccs_total)\n",
    "\n",
    "adj_matrix_total = np.zeros((len(labels_total),len(labels_total)))\n",
    "\n",
    "for i in tqdm(range(len(labels_total))):\n",
    "#     if i==1:\n",
    "    \n",
    "        dist1, nbors1 =neigh1.kneighbors([kpca_mfccs_total[i,:]])\n",
    "\n",
    "        #only keep train neighbor\n",
    "        nbors1 = nbors1[nbors1<len(labels_train)]\n",
    "        \n",
    "        neigh_total = np.array(nbors1).squeeze().flatten()\n",
    "#         print(neigh_total)\n",
    "        neigh_total = np.unique(neigh_total)\n",
    "#         print(neigh_total)\n",
    "        adj_matrix_total[i,neigh_total] = 1\n",
    "        adj_matrix_total[neigh_total,i] = 1\n",
    "\n",
    "        \n",
    "adj_matrix_total = np.zeros((len(labels_total),len(labels_total)))\n",
    "\n",
    "for i in tqdm(range(len(labels_total))):\n",
    "\n",
    "    dist1, nbors1 =neigh1.kneighbors([kpca_mfccs_total[i,:]])\n",
    "    #only keep train neighbor\n",
    "    dist1 = dist1[nbors1<len(labels_train)]\n",
    "    nbors1 = nbors1[nbors1<len(labels_train)]\n",
    "\n",
    "    n_total = []\n",
    "    n_total.append(nbors1)\n",
    "\n",
    "    dist_total = []\n",
    "    dist_total.append(dist1)\n",
    "\n",
    "    neigh_total = np.array(n_total).squeeze().flatten()\n",
    "    dist_total = np.array(dist_total).squeeze().flatten()\n",
    "\n",
    "    neigh_total = np.unique(neigh_total)\n",
    "\n",
    "    for j in range(len(neigh_total)):\n",
    "        adj_matrix_total[i,neigh_total[j]] = dist_total[j]\n",
    "        adj_matrix_total[neigh_total[j],i] = dist_total[j]\n",
    "        \n",
    "#train graph\n",
    "nx_graph_train = nx.from_numpy_matrix(adj_matrix_total[0:len(labels_train),0:len(labels_train)])\n",
    "g_dgl_euclid_train = dgl.from_networkx(nx_graph_train)\n",
    "g_dgl_euclid_train\n",
    "\n",
    "#test graph\n",
    "nx_graph_test = nx.from_numpy_matrix(adj_matrix_total[len(labels_train):,len(labels_train):])\n",
    "g_dgl_euclid_test = dgl.from_networkx(nx_graph_test)\n",
    "g_dgl_euclid_test\n",
    "\n",
    "#Total Graph\n",
    "nx_graph_total= nx.from_numpy_matrix(adj_matrix_total)\n",
    "g_dgl_euclid_total = dgl.from_networkx(nx_graph_total)\n",
    "g_dgl_euclid_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf3c5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "gcn_reduce\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "def metrics_summary(val, pred):\n",
    "    sens = recall_score(val, pred)\n",
    "    spec = recall_score(1-val, 1-pred)\n",
    "    prec = precision_score(val, pred)\n",
    "    harmonic = 2*spec*sens/(sens+spec)\n",
    "\n",
    "    print(\"prec: \", prec)\n",
    "    print(\"sens: \", sens)\n",
    "    print(\"spec: \", spec)\n",
    "    print(\"harmonic: \", harmonic)\n",
    "    \n",
    "###create train and valid\n",
    "train_len = len(labels_train)\n",
    "train_mask_full = np.full(len(labels_total),True)\n",
    "train_mask_full[train_len:]= False\n",
    "valid_mask_full = ~train_mask_full\n",
    "t_mask = th.BoolTensor(train_mask_full)\n",
    "v_mask = th.BoolTensor(valid_mask_full)\n",
    "inputs_g_total = torch.tensor(kpca_mfccs_total).float()\n",
    "labels_graph = torch.tensor(labels_total).long()\n",
    "\n",
    "###create train and valid\n",
    "\n",
    "train_mask = np.full(len(labels_train),True)\n",
    "t_mask2 = th.BoolTensor(train_mask)\n",
    "\n",
    "test_mask = np.full(len(labels_test),True)\n",
    "t_mask_test = th.BoolTensor(test_mask)\n",
    "\n",
    "\n",
    "inputs_g_train = torch.tensor(kpca_mfccs_total[:len(labels_train)]).float()\n",
    "labels_graph_train = torch.tensor(labels_train).long()\n",
    "\n",
    "inputs_g_test = torch.tensor(kpca_mfccs_total[len(labels_train):]).float()\n",
    "labels_graph_test = torch.tensor(labels_test).long()\n",
    "\n",
    "\n",
    "train_mask_total = np.full(len(labels_total),True)\n",
    "t_mask_total = th.BoolTensor(train_mask_total)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e561923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.0138 | Train Acc 0.5604| Test Acc 0.5424\n",
      "Epoch 00001 | Loss 0.8174 | Train Acc 0.5903| Test Acc 0.5606\n",
      "Epoch 00002 | Loss 0.7200 | Train Acc 0.6246| Test Acc 0.6045\n",
      "Epoch 00003 | Loss 0.6496 | Train Acc 0.6396| Test Acc 0.6439\n",
      "Epoch 00004 | Loss 0.5974 | Train Acc 0.6649| Test Acc 0.6864\n",
      "Epoch 00005 | Loss 0.5558 | Train Acc 0.7112| Test Acc 0.7227\n",
      "Epoch 00006 | Loss 0.5189 | Train Acc 0.7515| Test Acc 0.7621\n",
      "Epoch 00007 | Loss 0.4839 | Train Acc 0.7828| Test Acc 0.7864\n",
      "Epoch 00008 | Loss 0.4501 | Train Acc 0.8067| Test Acc 0.7985\n",
      "Epoch 00009 | Loss 0.4177 | Train Acc 0.8284| Test Acc 0.8152\n",
      "Epoch 00010 | Loss 0.3880 | Train Acc 0.8381| Test Acc 0.8318\n",
      "Epoch 00011 | Loss 0.3615 | Train Acc 0.8448| Test Acc 0.8515\n",
      "Epoch 00012 | Loss 0.3386 | Train Acc 0.8672| Test Acc 0.8576\n",
      "Epoch 00013 | Loss 0.3192 | Train Acc 0.8731| Test Acc 0.8742\n",
      "Epoch 00014 | Loss 0.3027 | Train Acc 0.8821| Test Acc 0.8788\n",
      "Epoch 00015 | Loss 0.2884 | Train Acc 0.8888| Test Acc 0.8909\n",
      "Epoch 00016 | Loss 0.2759 | Train Acc 0.8940| Test Acc 0.8970\n",
      "Epoch 00017 | Loss 0.2646 | Train Acc 0.9000| Test Acc 0.9076\n",
      "Epoch 00018 | Loss 0.2541 | Train Acc 0.9052| Test Acc 0.9106\n",
      "Epoch 00019 | Loss 0.2441 | Train Acc 0.9097| Test Acc 0.9091\n",
      "Epoch 00020 | Loss 0.2345 | Train Acc 0.9134| Test Acc 0.9197\n",
      "Epoch 00021 | Loss 0.2252 | Train Acc 0.9157| Test Acc 0.9242\n",
      "Epoch 00022 | Loss 0.2164 | Train Acc 0.9179| Test Acc 0.9258\n",
      "Epoch 00023 | Loss 0.2081 | Train Acc 0.9216| Test Acc 0.9303\n",
      "Epoch 00024 | Loss 0.2004 | Train Acc 0.9254| Test Acc 0.9333\n",
      "Epoch 00025 | Loss 0.1933 | Train Acc 0.9306| Test Acc 0.9348\n",
      "Epoch 00026 | Loss 0.1867 | Train Acc 0.9358| Test Acc 0.9379\n",
      "Epoch 00027 | Loss 0.1807 | Train Acc 0.9381| Test Acc 0.9379\n",
      "Epoch 00028 | Loss 0.1752 | Train Acc 0.9410| Test Acc 0.9364\n",
      "Epoch 00029 | Loss 0.1699 | Train Acc 0.9418| Test Acc 0.9364\n",
      "Epoch 00030 | Loss 0.1650 | Train Acc 0.9433| Test Acc 0.9379\n",
      "Epoch 00031 | Loss 0.1602 | Train Acc 0.9440| Test Acc 0.9379\n",
      "Epoch 00032 | Loss 0.1555 | Train Acc 0.9463| Test Acc 0.9394\n",
      "Epoch 00033 | Loss 0.1510 | Train Acc 0.9493| Test Acc 0.9394\n",
      "Epoch 00034 | Loss 0.1466 | Train Acc 0.9515| Test Acc 0.9379\n",
      "Epoch 00035 | Loss 0.1423 | Train Acc 0.9545| Test Acc 0.9379\n",
      "Epoch 00036 | Loss 0.1383 | Train Acc 0.9552| Test Acc 0.9439\n",
      "Epoch 00037 | Loss 0.1344 | Train Acc 0.9560| Test Acc 0.9455\n",
      "Epoch 00038 | Loss 0.1307 | Train Acc 0.9575| Test Acc 0.9455\n",
      "Epoch 00039 | Loss 0.1273 | Train Acc 0.9612| Test Acc 0.9470\n",
      "Epoch 00040 | Loss 0.1240 | Train Acc 0.9627| Test Acc 0.9470\n",
      "Epoch 00041 | Loss 0.1209 | Train Acc 0.9657| Test Acc 0.9455\n",
      "Epoch 00042 | Loss 0.1179 | Train Acc 0.9664| Test Acc 0.9439\n",
      "Epoch 00043 | Loss 0.1151 | Train Acc 0.9664| Test Acc 0.9470\n",
      "Epoch 00044 | Loss 0.1124 | Train Acc 0.9679| Test Acc 0.9485\n",
      "Epoch 00045 | Loss 0.1097 | Train Acc 0.9679| Test Acc 0.9485\n",
      "Epoch 00046 | Loss 0.1071 | Train Acc 0.9687| Test Acc 0.9500\n",
      "Epoch 00047 | Loss 0.1046 | Train Acc 0.9687| Test Acc 0.9515\n",
      "Epoch 00048 | Loss 0.1022 | Train Acc 0.9687| Test Acc 0.9530\n",
      "Epoch 00049 | Loss 0.0999 | Train Acc 0.9716| Test Acc 0.9530\n",
      "Epoch 00050 | Loss 0.0976 | Train Acc 0.9716| Test Acc 0.9530\n",
      "Epoch 00051 | Loss 0.0954 | Train Acc 0.9731| Test Acc 0.9545\n",
      "Epoch 00052 | Loss 0.0933 | Train Acc 0.9731| Test Acc 0.9545\n",
      "Epoch 00053 | Loss 0.0913 | Train Acc 0.9731| Test Acc 0.9545\n",
      "Epoch 00054 | Loss 0.0893 | Train Acc 0.9731| Test Acc 0.9545\n",
      "Epoch 00055 | Loss 0.0873 | Train Acc 0.9746| Test Acc 0.9561\n",
      "Epoch 00056 | Loss 0.0854 | Train Acc 0.9769| Test Acc 0.9561\n",
      "Epoch 00057 | Loss 0.0836 | Train Acc 0.9776| Test Acc 0.9576\n",
      "Epoch 00058 | Loss 0.0817 | Train Acc 0.9784| Test Acc 0.9576\n",
      "Epoch 00059 | Loss 0.0800 | Train Acc 0.9784| Test Acc 0.9576\n",
      "Epoch 00060 | Loss 0.0782 | Train Acc 0.9784| Test Acc 0.9591\n",
      "Epoch 00061 | Loss 0.0765 | Train Acc 0.9784| Test Acc 0.9591\n",
      "Epoch 00062 | Loss 0.0749 | Train Acc 0.9791| Test Acc 0.9606\n",
      "Epoch 00063 | Loss 0.0733 | Train Acc 0.9799| Test Acc 0.9606\n",
      "Epoch 00064 | Loss 0.0717 | Train Acc 0.9799| Test Acc 0.9606\n",
      "Epoch 00065 | Loss 0.0702 | Train Acc 0.9813| Test Acc 0.9591\n",
      "Epoch 00066 | Loss 0.0687 | Train Acc 0.9821| Test Acc 0.9576\n",
      "Epoch 00067 | Loss 0.0672 | Train Acc 0.9828| Test Acc 0.9576\n",
      "Epoch 00068 | Loss 0.0658 | Train Acc 0.9843| Test Acc 0.9576\n",
      "Epoch 00069 | Loss 0.0644 | Train Acc 0.9843| Test Acc 0.9576\n",
      "Epoch 00070 | Loss 0.0631 | Train Acc 0.9843| Test Acc 0.9576\n",
      "Epoch 00071 | Loss 0.0618 | Train Acc 0.9843| Test Acc 0.9591\n",
      "Epoch 00072 | Loss 0.0605 | Train Acc 0.9843| Test Acc 0.9591\n",
      "Epoch 00073 | Loss 0.0592 | Train Acc 0.9858| Test Acc 0.9591\n",
      "Epoch 00074 | Loss 0.0580 | Train Acc 0.9866| Test Acc 0.9591\n",
      "Epoch 00075 | Loss 0.0568 | Train Acc 0.9873| Test Acc 0.9591\n",
      "Epoch 00076 | Loss 0.0557 | Train Acc 0.9873| Test Acc 0.9591\n",
      "Epoch 00077 | Loss 0.0545 | Train Acc 0.9873| Test Acc 0.9621\n",
      "Epoch 00078 | Loss 0.0534 | Train Acc 0.9873| Test Acc 0.9621\n",
      "Epoch 00079 | Loss 0.0523 | Train Acc 0.9873| Test Acc 0.9621\n",
      "Epoch 00080 | Loss 0.0513 | Train Acc 0.9873| Test Acc 0.9621\n",
      "Epoch 00081 | Loss 0.0502 | Train Acc 0.9881| Test Acc 0.9621\n",
      "Epoch 00082 | Loss 0.0492 | Train Acc 0.9888| Test Acc 0.9636\n",
      "Epoch 00083 | Loss 0.0482 | Train Acc 0.9888| Test Acc 0.9621\n",
      "Epoch 00084 | Loss 0.0473 | Train Acc 0.9888| Test Acc 0.9621\n",
      "Epoch 00085 | Loss 0.0463 | Train Acc 0.9896| Test Acc 0.9621\n",
      "Epoch 00086 | Loss 0.0454 | Train Acc 0.9903| Test Acc 0.9621\n",
      "Epoch 00087 | Loss 0.0445 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00088 | Loss 0.0436 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00089 | Loss 0.0428 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00090 | Loss 0.0419 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00091 | Loss 0.0411 | Train Acc 0.9918| Test Acc 0.9652\n",
      "Epoch 00092 | Loss 0.0403 | Train Acc 0.9918| Test Acc 0.9652\n",
      "Epoch 00093 | Loss 0.0396 | Train Acc 0.9925| Test Acc 0.9652\n",
      "Epoch 00094 | Loss 0.0388 | Train Acc 0.9933| Test Acc 0.9652\n",
      "Epoch 00095 | Loss 0.0381 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00096 | Loss 0.0373 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00097 | Loss 0.0366 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00098 | Loss 0.0359 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00099 | Loss 0.0353 | Train Acc 0.9948| Test Acc 0.9636\n",
      "Epoch 00100 | Loss 0.0346 | Train Acc 0.9948| Test Acc 0.9652\n",
      "Epoch 00101 | Loss 0.0339 | Train Acc 0.9948| Test Acc 0.9652\n",
      "Epoch 00102 | Loss 0.0333 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00103 | Loss 0.0327 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00104 | Loss 0.0321 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00105 | Loss 0.0315 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00106 | Loss 0.0309 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00107 | Loss 0.0303 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00108 | Loss 0.0298 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00109 | Loss 0.0292 | Train Acc 0.9963| Test Acc 0.9652\n",
      "Epoch 00110 | Loss 0.0287 | Train Acc 0.9970| Test Acc 0.9652\n",
      "Epoch 00111 | Loss 0.0282 | Train Acc 0.9970| Test Acc 0.9652\n",
      "Epoch 00112 | Loss 0.0277 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00113 | Loss 0.0272 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00114 | Loss 0.0267 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00115 | Loss 0.0262 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00116 | Loss 0.0257 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00117 | Loss 0.0253 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00118 | Loss 0.0248 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00119 | Loss 0.0244 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00120 | Loss 0.0240 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00121 | Loss 0.0236 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00122 | Loss 0.0232 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00123 | Loss 0.0228 | Train Acc 0.9978| Test Acc 0.9652\n",
      "Epoch 00124 | Loss 0.0224 | Train Acc 0.9978| Test Acc 0.9667\n",
      "Epoch 00125 | Loss 0.0220 | Train Acc 0.9978| Test Acc 0.9667\n",
      "Epoch 00126 | Loss 0.0216 | Train Acc 0.9978| Test Acc 0.9667\n",
      "Epoch 00127 | Loss 0.0213 | Train Acc 0.9978| Test Acc 0.9667\n",
      "Epoch 00128 | Loss 0.0209 | Train Acc 0.9978| Test Acc 0.9667\n",
      "Epoch 00129 | Loss 0.0206 | Train Acc 0.9985| Test Acc 0.9667\n",
      "Epoch 00130 | Loss 0.0202 | Train Acc 0.9985| Test Acc 0.9667\n",
      "Epoch 00131 | Loss 0.0199 | Train Acc 0.9985| Test Acc 0.9667\n",
      "Epoch 00132 | Loss 0.0195 | Train Acc 0.9985| Test Acc 0.9667\n",
      "Epoch 00133 | Loss 0.0192 | Train Acc 0.9993| Test Acc 0.9652\n",
      "Epoch 00134 | Loss 0.0189 | Train Acc 0.9993| Test Acc 0.9652\n",
      "Epoch 00135 | Loss 0.0186 | Train Acc 0.9993| Test Acc 0.9652\n",
      "Epoch 00136 | Loss 0.0183 | Train Acc 0.9993| Test Acc 0.9652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00137 | Loss 0.0180 | Train Acc 0.9993| Test Acc 0.9652\n",
      "Epoch 00138 | Loss 0.0177 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00139 | Loss 0.0174 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00140 | Loss 0.0171 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00141 | Loss 0.0169 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00142 | Loss 0.0166 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00143 | Loss 0.0163 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00144 | Loss 0.0161 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00145 | Loss 0.0158 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00146 | Loss 0.0155 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00147 | Loss 0.0153 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00148 | Loss 0.0151 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00149 | Loss 0.0148 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00150 | Loss 0.0146 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00151 | Loss 0.0144 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00152 | Loss 0.0141 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00153 | Loss 0.0139 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00154 | Loss 0.0137 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00155 | Loss 0.0135 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00156 | Loss 0.0133 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00157 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00158 | Loss 0.0129 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00159 | Loss 0.0127 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00160 | Loss 0.0125 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00161 | Loss 0.0123 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00162 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00163 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00164 | Loss 0.0117 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00165 | Loss 0.0115 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00166 | Loss 0.0113 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00167 | Loss 0.0112 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00168 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00169 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00170 | Loss 0.0107 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00171 | Loss 0.0105 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00172 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00173 | Loss 0.0102 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00174 | Loss 0.0101 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00175 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00176 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00177 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00178 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00179 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00180 | Loss 0.0092 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00181 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00182 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00183 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00184 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00185 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00186 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00187 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00188 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00189 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00190 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00191 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00192 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00193 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00194 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00195 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00196 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00197 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00198 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00199 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9697\n"
     ]
    }
   ],
   "source": [
    "# Contruct a two-layer GNN model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(\n",
    "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean',feat_drop=0.05)\n",
    "        self.conv2 = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats, out_feats=hid_feats, aggregator_type='mean',feat_drop=0.05)\n",
    "#         self.conv4 = dglnn.SAGEConv(\n",
    "#             in_feats=hid_feats, out_feats=hid_feats, aggregator_type='mean',feat_drop=0.01)\n",
    "        self.conv3 = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        \n",
    "#         h = self.conv2(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv2(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv2(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv2(graph, h)\n",
    "#         h = F.relu(h)\n",
    "        \n",
    "#         h = self.conv4(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv4(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv4(graph, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv4(graph, h)\n",
    "#         h = F.relu(h)\n",
    "        \n",
    "        h = self.conv3(graph, h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "model_gsage = SAGE(in_feats=58, hid_feats=80, out_feats=2)\n",
    "opt = th.optim.Adam(model_gsage.parameters(),lr=0.001)\n",
    "model_gsage.train()\n",
    "loss_list=[]\n",
    "acc_list=[]\n",
    "acc_test_list=[]\n",
    "for epoch in range(200):\n",
    "    out = model_gsage(g_dgl_euclid_train, inputs_g_train)\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(out, labels_graph_train)\n",
    "    # compute validation accuracy\n",
    "    acc = evaluate(model_gsage, g_dgl_euclid_train, inputs_g_train, labels_graph_train, t_mask2)\n",
    "    acc_test=evaluate(model_gsage, g_dgl_euclid_test, inputs_g_test, labels_graph_test, t_mask_test)\n",
    "    loss_list.append(loss.detach().numpy())\n",
    "    acc_list.append(acc)\n",
    "    acc_test_list.append(acc_test)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Train Acc {:.4f}| Test Acc {:.4f}\".format(\n",
    "            epoch, loss.item(), acc,acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78c174b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_list,label='Train Accuracy')\n",
    "# plt.plot(loss_list,label='Loss')\n",
    "plt.plot(acc_test_list,label='Test Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Curve\\n Binary Classification with 2000 Samples')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a31a1f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.4874 | Train Acc 0.3463| Test Acc 0.3788\n",
      "Epoch 00001 | Loss 1.2173 | Train Acc 0.3701| Test Acc 0.4242\n",
      "Epoch 00002 | Loss 1.0374 | Train Acc 0.4157| Test Acc 0.4576\n",
      "Epoch 00003 | Loss 0.8910 | Train Acc 0.4806| Test Acc 0.5061\n",
      "Epoch 00004 | Loss 0.7754 | Train Acc 0.5381| Test Acc 0.5788\n",
      "Epoch 00005 | Loss 0.6842 | Train Acc 0.6045| Test Acc 0.6500\n",
      "Epoch 00006 | Loss 0.6113 | Train Acc 0.6761| Test Acc 0.7121\n",
      "Epoch 00007 | Loss 0.5516 | Train Acc 0.7149| Test Acc 0.7621\n",
      "Epoch 00008 | Loss 0.5020 | Train Acc 0.7590| Test Acc 0.7924\n",
      "Epoch 00009 | Loss 0.4608 | Train Acc 0.7851| Test Acc 0.8167\n",
      "Epoch 00010 | Loss 0.4264 | Train Acc 0.8119| Test Acc 0.8333\n",
      "Epoch 00011 | Loss 0.3980 | Train Acc 0.8231| Test Acc 0.8530\n",
      "Epoch 00012 | Loss 0.3746 | Train Acc 0.8448| Test Acc 0.8652\n",
      "Epoch 00013 | Loss 0.3552 | Train Acc 0.8560| Test Acc 0.8742\n",
      "Epoch 00014 | Loss 0.3388 | Train Acc 0.8642| Test Acc 0.8773\n",
      "Epoch 00015 | Loss 0.3247 | Train Acc 0.8679| Test Acc 0.8864\n",
      "Epoch 00016 | Loss 0.3120 | Train Acc 0.8687| Test Acc 0.8924\n",
      "Epoch 00017 | Loss 0.3004 | Train Acc 0.8746| Test Acc 0.8985\n",
      "Epoch 00018 | Loss 0.2896 | Train Acc 0.8828| Test Acc 0.9000\n",
      "Epoch 00019 | Loss 0.2793 | Train Acc 0.8881| Test Acc 0.9061\n",
      "Epoch 00020 | Loss 0.2694 | Train Acc 0.8918| Test Acc 0.9076\n",
      "Epoch 00021 | Loss 0.2601 | Train Acc 0.8978| Test Acc 0.9091\n",
      "Epoch 00022 | Loss 0.2513 | Train Acc 0.9007| Test Acc 0.9106\n",
      "Epoch 00023 | Loss 0.2431 | Train Acc 0.9082| Test Acc 0.9106\n",
      "Epoch 00024 | Loss 0.2355 | Train Acc 0.9119| Test Acc 0.9136\n",
      "Epoch 00025 | Loss 0.2285 | Train Acc 0.9164| Test Acc 0.9167\n",
      "Epoch 00026 | Loss 0.2221 | Train Acc 0.9209| Test Acc 0.9197\n",
      "Epoch 00027 | Loss 0.2161 | Train Acc 0.9254| Test Acc 0.9197\n",
      "Epoch 00028 | Loss 0.2105 | Train Acc 0.9239| Test Acc 0.9197\n",
      "Epoch 00029 | Loss 0.2052 | Train Acc 0.9254| Test Acc 0.9212\n",
      "Epoch 00030 | Loss 0.2002 | Train Acc 0.9291| Test Acc 0.9242\n",
      "Epoch 00031 | Loss 0.1953 | Train Acc 0.9328| Test Acc 0.9288\n",
      "Epoch 00032 | Loss 0.1906 | Train Acc 0.9343| Test Acc 0.9318\n",
      "Epoch 00033 | Loss 0.1861 | Train Acc 0.9358| Test Acc 0.9333\n",
      "Epoch 00034 | Loss 0.1817 | Train Acc 0.9358| Test Acc 0.9379\n",
      "Epoch 00035 | Loss 0.1774 | Train Acc 0.9373| Test Acc 0.9394\n",
      "Epoch 00036 | Loss 0.1733 | Train Acc 0.9388| Test Acc 0.9394\n",
      "Epoch 00037 | Loss 0.1693 | Train Acc 0.9388| Test Acc 0.9394\n",
      "Epoch 00038 | Loss 0.1655 | Train Acc 0.9418| Test Acc 0.9409\n",
      "Epoch 00039 | Loss 0.1618 | Train Acc 0.9425| Test Acc 0.9409\n",
      "Epoch 00040 | Loss 0.1583 | Train Acc 0.9425| Test Acc 0.9409\n",
      "Epoch 00041 | Loss 0.1548 | Train Acc 0.9440| Test Acc 0.9394\n",
      "Epoch 00042 | Loss 0.1515 | Train Acc 0.9478| Test Acc 0.9394\n",
      "Epoch 00043 | Loss 0.1483 | Train Acc 0.9493| Test Acc 0.9394\n",
      "Epoch 00044 | Loss 0.1452 | Train Acc 0.9507| Test Acc 0.9394\n",
      "Epoch 00045 | Loss 0.1422 | Train Acc 0.9515| Test Acc 0.9424\n",
      "Epoch 00046 | Loss 0.1393 | Train Acc 0.9515| Test Acc 0.9455\n",
      "Epoch 00047 | Loss 0.1364 | Train Acc 0.9552| Test Acc 0.9455\n",
      "Epoch 00048 | Loss 0.1336 | Train Acc 0.9552| Test Acc 0.9470\n",
      "Epoch 00049 | Loss 0.1309 | Train Acc 0.9560| Test Acc 0.9470\n",
      "Epoch 00050 | Loss 0.1283 | Train Acc 0.9575| Test Acc 0.9470\n",
      "Epoch 00051 | Loss 0.1258 | Train Acc 0.9582| Test Acc 0.9470\n",
      "Epoch 00052 | Loss 0.1234 | Train Acc 0.9582| Test Acc 0.9470\n",
      "Epoch 00053 | Loss 0.1210 | Train Acc 0.9597| Test Acc 0.9470\n",
      "Epoch 00054 | Loss 0.1187 | Train Acc 0.9604| Test Acc 0.9470\n",
      "Epoch 00055 | Loss 0.1165 | Train Acc 0.9619| Test Acc 0.9485\n",
      "Epoch 00056 | Loss 0.1143 | Train Acc 0.9634| Test Acc 0.9500\n",
      "Epoch 00057 | Loss 0.1121 | Train Acc 0.9642| Test Acc 0.9485\n",
      "Epoch 00058 | Loss 0.1100 | Train Acc 0.9657| Test Acc 0.9455\n",
      "Epoch 00059 | Loss 0.1080 | Train Acc 0.9672| Test Acc 0.9455\n",
      "Epoch 00060 | Loss 0.1060 | Train Acc 0.9679| Test Acc 0.9455\n",
      "Epoch 00061 | Loss 0.1040 | Train Acc 0.9687| Test Acc 0.9455\n",
      "Epoch 00062 | Loss 0.1021 | Train Acc 0.9694| Test Acc 0.9455\n",
      "Epoch 00063 | Loss 0.1003 | Train Acc 0.9709| Test Acc 0.9455\n",
      "Epoch 00064 | Loss 0.0985 | Train Acc 0.9716| Test Acc 0.9455\n",
      "Epoch 00065 | Loss 0.0967 | Train Acc 0.9716| Test Acc 0.9455\n",
      "Epoch 00066 | Loss 0.0950 | Train Acc 0.9716| Test Acc 0.9470\n",
      "Epoch 00067 | Loss 0.0933 | Train Acc 0.9724| Test Acc 0.9470\n",
      "Epoch 00068 | Loss 0.0916 | Train Acc 0.9754| Test Acc 0.9470\n",
      "Epoch 00069 | Loss 0.0900 | Train Acc 0.9761| Test Acc 0.9470\n",
      "Epoch 00070 | Loss 0.0884 | Train Acc 0.9776| Test Acc 0.9470\n",
      "Epoch 00071 | Loss 0.0868 | Train Acc 0.9784| Test Acc 0.9470\n",
      "Epoch 00072 | Loss 0.0853 | Train Acc 0.9784| Test Acc 0.9485\n",
      "Epoch 00073 | Loss 0.0838 | Train Acc 0.9784| Test Acc 0.9485\n",
      "Epoch 00074 | Loss 0.0823 | Train Acc 0.9784| Test Acc 0.9485\n",
      "Epoch 00075 | Loss 0.0809 | Train Acc 0.9784| Test Acc 0.9485\n",
      "Epoch 00076 | Loss 0.0795 | Train Acc 0.9784| Test Acc 0.9500\n",
      "Epoch 00077 | Loss 0.0781 | Train Acc 0.9784| Test Acc 0.9500\n",
      "Epoch 00078 | Loss 0.0767 | Train Acc 0.9791| Test Acc 0.9500\n",
      "Epoch 00079 | Loss 0.0754 | Train Acc 0.9791| Test Acc 0.9485\n",
      "Epoch 00080 | Loss 0.0741 | Train Acc 0.9806| Test Acc 0.9485\n",
      "Epoch 00081 | Loss 0.0729 | Train Acc 0.9821| Test Acc 0.9485\n",
      "Epoch 00082 | Loss 0.0716 | Train Acc 0.9821| Test Acc 0.9485\n",
      "Epoch 00083 | Loss 0.0704 | Train Acc 0.9828| Test Acc 0.9485\n",
      "Epoch 00084 | Loss 0.0692 | Train Acc 0.9836| Test Acc 0.9500\n",
      "Epoch 00085 | Loss 0.0680 | Train Acc 0.9836| Test Acc 0.9500\n",
      "Epoch 00086 | Loss 0.0669 | Train Acc 0.9836| Test Acc 0.9500\n",
      "Epoch 00087 | Loss 0.0658 | Train Acc 0.9836| Test Acc 0.9500\n",
      "Epoch 00088 | Loss 0.0646 | Train Acc 0.9836| Test Acc 0.9545\n",
      "Epoch 00089 | Loss 0.0636 | Train Acc 0.9843| Test Acc 0.9561\n",
      "Epoch 00090 | Loss 0.0625 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00091 | Loss 0.0614 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00092 | Loss 0.0604 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00093 | Loss 0.0593 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00094 | Loss 0.0583 | Train Acc 0.9858| Test Acc 0.9606\n",
      "Epoch 00095 | Loss 0.0574 | Train Acc 0.9851| Test Acc 0.9606\n",
      "Epoch 00096 | Loss 0.0564 | Train Acc 0.9858| Test Acc 0.9606\n",
      "Epoch 00097 | Loss 0.0554 | Train Acc 0.9866| Test Acc 0.9606\n",
      "Epoch 00098 | Loss 0.0545 | Train Acc 0.9873| Test Acc 0.9621\n",
      "Epoch 00099 | Loss 0.0536 | Train Acc 0.9888| Test Acc 0.9636\n",
      "Epoch 00100 | Loss 0.0526 | Train Acc 0.9888| Test Acc 0.9636\n",
      "Epoch 00101 | Loss 0.0517 | Train Acc 0.9888| Test Acc 0.9652\n",
      "Epoch 00102 | Loss 0.0509 | Train Acc 0.9888| Test Acc 0.9652\n",
      "Epoch 00103 | Loss 0.0500 | Train Acc 0.9888| Test Acc 0.9652\n",
      "Epoch 00104 | Loss 0.0492 | Train Acc 0.9888| Test Acc 0.9652\n",
      "Epoch 00105 | Loss 0.0483 | Train Acc 0.9888| Test Acc 0.9652\n",
      "Epoch 00106 | Loss 0.0475 | Train Acc 0.9888| Test Acc 0.9667\n",
      "Epoch 00107 | Loss 0.0467 | Train Acc 0.9888| Test Acc 0.9667\n",
      "Epoch 00108 | Loss 0.0459 | Train Acc 0.9888| Test Acc 0.9667\n",
      "Epoch 00109 | Loss 0.0451 | Train Acc 0.9888| Test Acc 0.9667\n",
      "Epoch 00110 | Loss 0.0444 | Train Acc 0.9888| Test Acc 0.9667\n",
      "Epoch 00111 | Loss 0.0436 | Train Acc 0.9888| Test Acc 0.9682\n",
      "Epoch 00112 | Loss 0.0429 | Train Acc 0.9896| Test Acc 0.9682\n",
      "Epoch 00113 | Loss 0.0422 | Train Acc 0.9903| Test Acc 0.9682\n",
      "Epoch 00114 | Loss 0.0414 | Train Acc 0.9910| Test Acc 0.9682\n",
      "Epoch 00115 | Loss 0.0407 | Train Acc 0.9910| Test Acc 0.9682\n",
      "Epoch 00116 | Loss 0.0401 | Train Acc 0.9910| Test Acc 0.9682\n",
      "Epoch 00117 | Loss 0.0394 | Train Acc 0.9910| Test Acc 0.9682\n",
      "Epoch 00118 | Loss 0.0387 | Train Acc 0.9910| Test Acc 0.9667\n",
      "Epoch 00119 | Loss 0.0381 | Train Acc 0.9918| Test Acc 0.9667\n",
      "Epoch 00120 | Loss 0.0374 | Train Acc 0.9918| Test Acc 0.9667\n",
      "Epoch 00121 | Loss 0.0368 | Train Acc 0.9918| Test Acc 0.9667\n",
      "Epoch 00122 | Loss 0.0362 | Train Acc 0.9925| Test Acc 0.9667\n",
      "Epoch 00123 | Loss 0.0356 | Train Acc 0.9925| Test Acc 0.9667\n",
      "Epoch 00124 | Loss 0.0350 | Train Acc 0.9940| Test Acc 0.9667\n",
      "Epoch 00125 | Loss 0.0344 | Train Acc 0.9940| Test Acc 0.9667\n",
      "Epoch 00126 | Loss 0.0338 | Train Acc 0.9940| Test Acc 0.9667\n",
      "Epoch 00127 | Loss 0.0332 | Train Acc 0.9940| Test Acc 0.9667\n",
      "Epoch 00128 | Loss 0.0327 | Train Acc 0.9940| Test Acc 0.9667\n",
      "Epoch 00129 | Loss 0.0322 | Train Acc 0.9948| Test Acc 0.9667\n",
      "Epoch 00130 | Loss 0.0316 | Train Acc 0.9948| Test Acc 0.9667\n",
      "Epoch 00131 | Loss 0.0311 | Train Acc 0.9948| Test Acc 0.9667\n",
      "Epoch 00132 | Loss 0.0306 | Train Acc 0.9948| Test Acc 0.9667\n",
      "Epoch 00133 | Loss 0.0301 | Train Acc 0.9955| Test Acc 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00134 | Loss 0.0296 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00135 | Loss 0.0291 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00136 | Loss 0.0286 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00137 | Loss 0.0282 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00138 | Loss 0.0277 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00139 | Loss 0.0272 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00140 | Loss 0.0268 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00141 | Loss 0.0264 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00142 | Loss 0.0259 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00143 | Loss 0.0255 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00144 | Loss 0.0251 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00145 | Loss 0.0247 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00146 | Loss 0.0243 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00147 | Loss 0.0239 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00148 | Loss 0.0235 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00149 | Loss 0.0231 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00150 | Loss 0.0227 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00151 | Loss 0.0224 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00152 | Loss 0.0220 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00153 | Loss 0.0217 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00154 | Loss 0.0213 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00155 | Loss 0.0210 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00156 | Loss 0.0206 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00157 | Loss 0.0203 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00158 | Loss 0.0200 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00159 | Loss 0.0197 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00160 | Loss 0.0194 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00161 | Loss 0.0191 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00162 | Loss 0.0188 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00163 | Loss 0.0185 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00164 | Loss 0.0182 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00165 | Loss 0.0179 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00166 | Loss 0.0176 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00167 | Loss 0.0174 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00168 | Loss 0.0171 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00169 | Loss 0.0168 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00170 | Loss 0.0166 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00171 | Loss 0.0163 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00172 | Loss 0.0161 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00173 | Loss 0.0158 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00174 | Loss 0.0156 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00175 | Loss 0.0154 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00176 | Loss 0.0151 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00177 | Loss 0.0149 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00178 | Loss 0.0147 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00179 | Loss 0.0145 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00180 | Loss 0.0142 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00181 | Loss 0.0140 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00182 | Loss 0.0138 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00183 | Loss 0.0136 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00184 | Loss 0.0134 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00185 | Loss 0.0132 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00186 | Loss 0.0130 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00187 | Loss 0.0128 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00188 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00189 | Loss 0.0124 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00190 | Loss 0.0122 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00191 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00192 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00193 | Loss 0.0117 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00194 | Loss 0.0115 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00195 | Loss 0.0114 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00196 | Loss 0.0112 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00197 | Loss 0.0111 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00198 | Loss 0.0109 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00199 | Loss 0.0107 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00000 | Loss 1.5691 | Train Acc 0.4112| Test Acc 0.4242\n",
      "Epoch 00001 | Loss 1.2657 | Train Acc 0.4396| Test Acc 0.4545\n",
      "Epoch 00002 | Loss 1.0486 | Train Acc 0.4955| Test Acc 0.5091\n",
      "Epoch 00003 | Loss 0.8729 | Train Acc 0.5612| Test Acc 0.5833\n",
      "Epoch 00004 | Loss 0.7367 | Train Acc 0.6403| Test Acc 0.6333\n",
      "Epoch 00005 | Loss 0.6337 | Train Acc 0.6896| Test Acc 0.6894\n",
      "Epoch 00006 | Loss 0.5561 | Train Acc 0.7306| Test Acc 0.7136\n",
      "Epoch 00007 | Loss 0.4967 | Train Acc 0.7619| Test Acc 0.7545\n",
      "Epoch 00008 | Loss 0.4498 | Train Acc 0.7903| Test Acc 0.7697\n",
      "Epoch 00009 | Loss 0.4113 | Train Acc 0.8149| Test Acc 0.7848\n",
      "Epoch 00010 | Loss 0.3788 | Train Acc 0.8313| Test Acc 0.7939\n",
      "Epoch 00011 | Loss 0.3511 | Train Acc 0.8515| Test Acc 0.8121\n",
      "Epoch 00012 | Loss 0.3275 | Train Acc 0.8604| Test Acc 0.8288\n",
      "Epoch 00013 | Loss 0.3075 | Train Acc 0.8739| Test Acc 0.8455\n",
      "Epoch 00014 | Loss 0.2911 | Train Acc 0.8866| Test Acc 0.8606\n",
      "Epoch 00015 | Loss 0.2777 | Train Acc 0.8963| Test Acc 0.8667\n",
      "Epoch 00016 | Loss 0.2666 | Train Acc 0.9037| Test Acc 0.8712\n",
      "Epoch 00017 | Loss 0.2574 | Train Acc 0.9052| Test Acc 0.8773\n",
      "Epoch 00018 | Loss 0.2494 | Train Acc 0.9075| Test Acc 0.8773\n",
      "Epoch 00019 | Loss 0.2422 | Train Acc 0.9097| Test Acc 0.8803\n",
      "Epoch 00020 | Loss 0.2355 | Train Acc 0.9112| Test Acc 0.8894\n",
      "Epoch 00021 | Loss 0.2290 | Train Acc 0.9134| Test Acc 0.8894\n",
      "Epoch 00022 | Loss 0.2227 | Train Acc 0.9164| Test Acc 0.8955\n",
      "Epoch 00023 | Loss 0.2164 | Train Acc 0.9179| Test Acc 0.8985\n",
      "Epoch 00024 | Loss 0.2103 | Train Acc 0.9209| Test Acc 0.9015\n",
      "Epoch 00025 | Loss 0.2042 | Train Acc 0.9231| Test Acc 0.9015\n",
      "Epoch 00026 | Loss 0.1984 | Train Acc 0.9231| Test Acc 0.9015\n",
      "Epoch 00027 | Loss 0.1928 | Train Acc 0.9246| Test Acc 0.9076\n",
      "Epoch 00028 | Loss 0.1874 | Train Acc 0.9261| Test Acc 0.9091\n",
      "Epoch 00029 | Loss 0.1823 | Train Acc 0.9306| Test Acc 0.9091\n",
      "Epoch 00030 | Loss 0.1774 | Train Acc 0.9313| Test Acc 0.9121\n",
      "Epoch 00031 | Loss 0.1728 | Train Acc 0.9343| Test Acc 0.9152\n",
      "Epoch 00032 | Loss 0.1684 | Train Acc 0.9366| Test Acc 0.9167\n",
      "Epoch 00033 | Loss 0.1642 | Train Acc 0.9366| Test Acc 0.9197\n",
      "Epoch 00034 | Loss 0.1601 | Train Acc 0.9396| Test Acc 0.9227\n",
      "Epoch 00035 | Loss 0.1561 | Train Acc 0.9418| Test Acc 0.9227\n",
      "Epoch 00036 | Loss 0.1523 | Train Acc 0.9448| Test Acc 0.9227\n",
      "Epoch 00037 | Loss 0.1485 | Train Acc 0.9463| Test Acc 0.9227\n",
      "Epoch 00038 | Loss 0.1448 | Train Acc 0.9478| Test Acc 0.9227\n",
      "Epoch 00039 | Loss 0.1413 | Train Acc 0.9485| Test Acc 0.9227\n",
      "Epoch 00040 | Loss 0.1379 | Train Acc 0.9515| Test Acc 0.9227\n",
      "Epoch 00041 | Loss 0.1345 | Train Acc 0.9515| Test Acc 0.9227\n",
      "Epoch 00042 | Loss 0.1314 | Train Acc 0.9522| Test Acc 0.9242\n",
      "Epoch 00043 | Loss 0.1283 | Train Acc 0.9530| Test Acc 0.9258\n",
      "Epoch 00044 | Loss 0.1254 | Train Acc 0.9537| Test Acc 0.9273\n",
      "Epoch 00045 | Loss 0.1226 | Train Acc 0.9552| Test Acc 0.9273\n",
      "Epoch 00046 | Loss 0.1200 | Train Acc 0.9575| Test Acc 0.9273\n",
      "Epoch 00047 | Loss 0.1174 | Train Acc 0.9590| Test Acc 0.9288\n",
      "Epoch 00048 | Loss 0.1149 | Train Acc 0.9597| Test Acc 0.9288\n",
      "Epoch 00049 | Loss 0.1125 | Train Acc 0.9634| Test Acc 0.9288\n",
      "Epoch 00050 | Loss 0.1102 | Train Acc 0.9634| Test Acc 0.9303\n",
      "Epoch 00051 | Loss 0.1079 | Train Acc 0.9634| Test Acc 0.9318\n",
      "Epoch 00052 | Loss 0.1056 | Train Acc 0.9664| Test Acc 0.9318\n",
      "Epoch 00053 | Loss 0.1034 | Train Acc 0.9664| Test Acc 0.9333\n",
      "Epoch 00054 | Loss 0.1013 | Train Acc 0.9672| Test Acc 0.9333\n",
      "Epoch 00055 | Loss 0.0992 | Train Acc 0.9679| Test Acc 0.9333\n",
      "Epoch 00056 | Loss 0.0972 | Train Acc 0.9687| Test Acc 0.9348\n",
      "Epoch 00057 | Loss 0.0952 | Train Acc 0.9701| Test Acc 0.9348\n",
      "Epoch 00058 | Loss 0.0933 | Train Acc 0.9701| Test Acc 0.9333\n",
      "Epoch 00059 | Loss 0.0914 | Train Acc 0.9731| Test Acc 0.9333\n",
      "Epoch 00060 | Loss 0.0896 | Train Acc 0.9731| Test Acc 0.9333\n",
      "Epoch 00061 | Loss 0.0878 | Train Acc 0.9731| Test Acc 0.9333\n",
      "Epoch 00062 | Loss 0.0861 | Train Acc 0.9731| Test Acc 0.9318\n",
      "Epoch 00063 | Loss 0.0844 | Train Acc 0.9739| Test Acc 0.9318\n",
      "Epoch 00064 | Loss 0.0828 | Train Acc 0.9746| Test Acc 0.9303\n",
      "Epoch 00065 | Loss 0.0812 | Train Acc 0.9754| Test Acc 0.9303\n",
      "Epoch 00066 | Loss 0.0796 | Train Acc 0.9769| Test Acc 0.9303\n",
      "Epoch 00067 | Loss 0.0781 | Train Acc 0.9776| Test Acc 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00068 | Loss 0.0765 | Train Acc 0.9776| Test Acc 0.9333\n",
      "Epoch 00069 | Loss 0.0751 | Train Acc 0.9784| Test Acc 0.9333\n",
      "Epoch 00070 | Loss 0.0736 | Train Acc 0.9791| Test Acc 0.9333\n",
      "Epoch 00071 | Loss 0.0722 | Train Acc 0.9799| Test Acc 0.9348\n",
      "Epoch 00072 | Loss 0.0709 | Train Acc 0.9806| Test Acc 0.9364\n",
      "Epoch 00073 | Loss 0.0695 | Train Acc 0.9806| Test Acc 0.9364\n",
      "Epoch 00074 | Loss 0.0682 | Train Acc 0.9813| Test Acc 0.9364\n",
      "Epoch 00075 | Loss 0.0669 | Train Acc 0.9813| Test Acc 0.9394\n",
      "Epoch 00076 | Loss 0.0656 | Train Acc 0.9836| Test Acc 0.9409\n",
      "Epoch 00077 | Loss 0.0644 | Train Acc 0.9836| Test Acc 0.9409\n",
      "Epoch 00078 | Loss 0.0632 | Train Acc 0.9843| Test Acc 0.9409\n",
      "Epoch 00079 | Loss 0.0620 | Train Acc 0.9843| Test Acc 0.9409\n",
      "Epoch 00080 | Loss 0.0608 | Train Acc 0.9843| Test Acc 0.9424\n",
      "Epoch 00081 | Loss 0.0597 | Train Acc 0.9843| Test Acc 0.9424\n",
      "Epoch 00082 | Loss 0.0586 | Train Acc 0.9843| Test Acc 0.9439\n",
      "Epoch 00083 | Loss 0.0575 | Train Acc 0.9843| Test Acc 0.9439\n",
      "Epoch 00084 | Loss 0.0564 | Train Acc 0.9858| Test Acc 0.9455\n",
      "Epoch 00085 | Loss 0.0554 | Train Acc 0.9858| Test Acc 0.9455\n",
      "Epoch 00086 | Loss 0.0543 | Train Acc 0.9873| Test Acc 0.9470\n",
      "Epoch 00087 | Loss 0.0533 | Train Acc 0.9881| Test Acc 0.9470\n",
      "Epoch 00088 | Loss 0.0523 | Train Acc 0.9888| Test Acc 0.9485\n",
      "Epoch 00089 | Loss 0.0514 | Train Acc 0.9888| Test Acc 0.9500\n",
      "Epoch 00090 | Loss 0.0504 | Train Acc 0.9903| Test Acc 0.9500\n",
      "Epoch 00091 | Loss 0.0495 | Train Acc 0.9918| Test Acc 0.9530\n",
      "Epoch 00092 | Loss 0.0485 | Train Acc 0.9918| Test Acc 0.9530\n",
      "Epoch 00093 | Loss 0.0476 | Train Acc 0.9918| Test Acc 0.9530\n",
      "Epoch 00094 | Loss 0.0467 | Train Acc 0.9925| Test Acc 0.9545\n",
      "Epoch 00095 | Loss 0.0458 | Train Acc 0.9925| Test Acc 0.9561\n",
      "Epoch 00096 | Loss 0.0450 | Train Acc 0.9925| Test Acc 0.9561\n",
      "Epoch 00097 | Loss 0.0441 | Train Acc 0.9933| Test Acc 0.9561\n",
      "Epoch 00098 | Loss 0.0433 | Train Acc 0.9940| Test Acc 0.9561\n",
      "Epoch 00099 | Loss 0.0425 | Train Acc 0.9940| Test Acc 0.9561\n",
      "Epoch 00100 | Loss 0.0417 | Train Acc 0.9948| Test Acc 0.9545\n",
      "Epoch 00101 | Loss 0.0409 | Train Acc 0.9948| Test Acc 0.9530\n",
      "Epoch 00102 | Loss 0.0401 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00103 | Loss 0.0393 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00104 | Loss 0.0386 | Train Acc 0.9955| Test Acc 0.9530\n",
      "Epoch 00105 | Loss 0.0379 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00106 | Loss 0.0372 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00107 | Loss 0.0365 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00108 | Loss 0.0358 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00109 | Loss 0.0351 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00110 | Loss 0.0344 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00111 | Loss 0.0338 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00112 | Loss 0.0332 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00113 | Loss 0.0325 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00114 | Loss 0.0319 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00115 | Loss 0.0313 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00116 | Loss 0.0307 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00117 | Loss 0.0302 | Train Acc 0.9970| Test Acc 0.9576\n",
      "Epoch 00118 | Loss 0.0296 | Train Acc 0.9970| Test Acc 0.9576\n",
      "Epoch 00119 | Loss 0.0290 | Train Acc 0.9970| Test Acc 0.9576\n",
      "Epoch 00120 | Loss 0.0285 | Train Acc 0.9970| Test Acc 0.9576\n",
      "Epoch 00121 | Loss 0.0279 | Train Acc 0.9970| Test Acc 0.9591\n",
      "Epoch 00122 | Loss 0.0274 | Train Acc 0.9970| Test Acc 0.9591\n",
      "Epoch 00123 | Loss 0.0269 | Train Acc 0.9978| Test Acc 0.9591\n",
      "Epoch 00124 | Loss 0.0264 | Train Acc 0.9978| Test Acc 0.9591\n",
      "Epoch 00125 | Loss 0.0259 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00126 | Loss 0.0254 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00127 | Loss 0.0249 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00128 | Loss 0.0244 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00129 | Loss 0.0240 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00130 | Loss 0.0235 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00131 | Loss 0.0231 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00132 | Loss 0.0226 | Train Acc 0.9985| Test Acc 0.9621\n",
      "Epoch 00133 | Loss 0.0222 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00134 | Loss 0.0218 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00135 | Loss 0.0214 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00136 | Loss 0.0210 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00137 | Loss 0.0206 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00138 | Loss 0.0202 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00139 | Loss 0.0198 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00140 | Loss 0.0194 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00141 | Loss 0.0191 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00142 | Loss 0.0187 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00143 | Loss 0.0184 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00144 | Loss 0.0180 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00145 | Loss 0.0177 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00146 | Loss 0.0174 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00147 | Loss 0.0170 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00148 | Loss 0.0167 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00149 | Loss 0.0164 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00150 | Loss 0.0161 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00151 | Loss 0.0158 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00152 | Loss 0.0155 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00153 | Loss 0.0152 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00154 | Loss 0.0149 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00155 | Loss 0.0147 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00156 | Loss 0.0144 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00157 | Loss 0.0141 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00158 | Loss 0.0139 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00159 | Loss 0.0136 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00160 | Loss 0.0133 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00161 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00162 | Loss 0.0129 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00163 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00164 | Loss 0.0124 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00165 | Loss 0.0122 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00166 | Loss 0.0120 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00167 | Loss 0.0118 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00168 | Loss 0.0116 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00169 | Loss 0.0114 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00170 | Loss 0.0112 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00171 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00172 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00173 | Loss 0.0106 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00174 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00175 | Loss 0.0103 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00176 | Loss 0.0101 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00177 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00178 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00179 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00180 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00181 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00182 | Loss 0.0092 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00183 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00184 | Loss 0.0089 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00185 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00186 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00187 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00188 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00189 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00190 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00191 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00192 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00193 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00194 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00195 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00196 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00197 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00198 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00199 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00000 | Loss 1.2139 | Train Acc 0.3687| Test Acc 0.3545\n",
      "Epoch 00001 | Loss 1.0243 | Train Acc 0.4254| Test Acc 0.4333\n",
      "Epoch 00002 | Loss 0.8878 | Train Acc 0.5164| Test Acc 0.5076\n",
      "Epoch 00003 | Loss 0.7716 | Train Acc 0.5993| Test Acc 0.6000\n",
      "Epoch 00004 | Loss 0.6749 | Train Acc 0.6545| Test Acc 0.6591\n",
      "Epoch 00005 | Loss 0.5956 | Train Acc 0.7030| Test Acc 0.7242\n",
      "Epoch 00006 | Loss 0.5311 | Train Acc 0.7537| Test Acc 0.7561\n",
      "Epoch 00007 | Loss 0.4787 | Train Acc 0.7843| Test Acc 0.7742\n",
      "Epoch 00008 | Loss 0.4359 | Train Acc 0.8075| Test Acc 0.7909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009 | Loss 0.4004 | Train Acc 0.8388| Test Acc 0.8197\n",
      "Epoch 00010 | Loss 0.3705 | Train Acc 0.8552| Test Acc 0.8333\n",
      "Epoch 00011 | Loss 0.3449 | Train Acc 0.8664| Test Acc 0.8500\n",
      "Epoch 00012 | Loss 0.3226 | Train Acc 0.8724| Test Acc 0.8576\n",
      "Epoch 00013 | Loss 0.3031 | Train Acc 0.8828| Test Acc 0.8712\n",
      "Epoch 00014 | Loss 0.2858 | Train Acc 0.8881| Test Acc 0.8773\n",
      "Epoch 00015 | Loss 0.2705 | Train Acc 0.8970| Test Acc 0.8864\n",
      "Epoch 00016 | Loss 0.2568 | Train Acc 0.9045| Test Acc 0.8970\n",
      "Epoch 00017 | Loss 0.2447 | Train Acc 0.9097| Test Acc 0.8985\n",
      "Epoch 00018 | Loss 0.2338 | Train Acc 0.9157| Test Acc 0.9045\n",
      "Epoch 00019 | Loss 0.2241 | Train Acc 0.9187| Test Acc 0.9091\n",
      "Epoch 00020 | Loss 0.2152 | Train Acc 0.9239| Test Acc 0.9106\n",
      "Epoch 00021 | Loss 0.2071 | Train Acc 0.9261| Test Acc 0.9152\n",
      "Epoch 00022 | Loss 0.1997 | Train Acc 0.9306| Test Acc 0.9182\n",
      "Epoch 00023 | Loss 0.1928 | Train Acc 0.9306| Test Acc 0.9227\n",
      "Epoch 00024 | Loss 0.1863 | Train Acc 0.9336| Test Acc 0.9273\n",
      "Epoch 00025 | Loss 0.1802 | Train Acc 0.9351| Test Acc 0.9288\n",
      "Epoch 00026 | Loss 0.1744 | Train Acc 0.9388| Test Acc 0.9288\n",
      "Epoch 00027 | Loss 0.1690 | Train Acc 0.9381| Test Acc 0.9288\n",
      "Epoch 00028 | Loss 0.1639 | Train Acc 0.9396| Test Acc 0.9303\n",
      "Epoch 00029 | Loss 0.1590 | Train Acc 0.9425| Test Acc 0.9333\n",
      "Epoch 00030 | Loss 0.1545 | Train Acc 0.9433| Test Acc 0.9348\n",
      "Epoch 00031 | Loss 0.1502 | Train Acc 0.9463| Test Acc 0.9379\n",
      "Epoch 00032 | Loss 0.1461 | Train Acc 0.9470| Test Acc 0.9394\n",
      "Epoch 00033 | Loss 0.1423 | Train Acc 0.9478| Test Acc 0.9409\n",
      "Epoch 00034 | Loss 0.1388 | Train Acc 0.9500| Test Acc 0.9409\n",
      "Epoch 00035 | Loss 0.1354 | Train Acc 0.9522| Test Acc 0.9409\n",
      "Epoch 00036 | Loss 0.1322 | Train Acc 0.9560| Test Acc 0.9409\n",
      "Epoch 00037 | Loss 0.1292 | Train Acc 0.9575| Test Acc 0.9455\n",
      "Epoch 00038 | Loss 0.1264 | Train Acc 0.9590| Test Acc 0.9470\n",
      "Epoch 00039 | Loss 0.1236 | Train Acc 0.9612| Test Acc 0.9470\n",
      "Epoch 00040 | Loss 0.1210 | Train Acc 0.9612| Test Acc 0.9455\n",
      "Epoch 00041 | Loss 0.1185 | Train Acc 0.9627| Test Acc 0.9409\n",
      "Epoch 00042 | Loss 0.1160 | Train Acc 0.9642| Test Acc 0.9409\n",
      "Epoch 00043 | Loss 0.1137 | Train Acc 0.9642| Test Acc 0.9424\n",
      "Epoch 00044 | Loss 0.1113 | Train Acc 0.9642| Test Acc 0.9439\n",
      "Epoch 00045 | Loss 0.1091 | Train Acc 0.9649| Test Acc 0.9439\n",
      "Epoch 00046 | Loss 0.1069 | Train Acc 0.9649| Test Acc 0.9439\n",
      "Epoch 00047 | Loss 0.1048 | Train Acc 0.9664| Test Acc 0.9439\n",
      "Epoch 00048 | Loss 0.1028 | Train Acc 0.9664| Test Acc 0.9455\n",
      "Epoch 00049 | Loss 0.1007 | Train Acc 0.9672| Test Acc 0.9455\n",
      "Epoch 00050 | Loss 0.0988 | Train Acc 0.9687| Test Acc 0.9455\n",
      "Epoch 00051 | Loss 0.0968 | Train Acc 0.9687| Test Acc 0.9439\n",
      "Epoch 00052 | Loss 0.0949 | Train Acc 0.9701| Test Acc 0.9439\n",
      "Epoch 00053 | Loss 0.0931 | Train Acc 0.9709| Test Acc 0.9439\n",
      "Epoch 00054 | Loss 0.0913 | Train Acc 0.9716| Test Acc 0.9439\n",
      "Epoch 00055 | Loss 0.0895 | Train Acc 0.9731| Test Acc 0.9455\n",
      "Epoch 00056 | Loss 0.0878 | Train Acc 0.9731| Test Acc 0.9455\n",
      "Epoch 00057 | Loss 0.0861 | Train Acc 0.9739| Test Acc 0.9455\n",
      "Epoch 00058 | Loss 0.0844 | Train Acc 0.9739| Test Acc 0.9455\n",
      "Epoch 00059 | Loss 0.0827 | Train Acc 0.9746| Test Acc 0.9455\n",
      "Epoch 00060 | Loss 0.0811 | Train Acc 0.9754| Test Acc 0.9455\n",
      "Epoch 00061 | Loss 0.0795 | Train Acc 0.9769| Test Acc 0.9455\n",
      "Epoch 00062 | Loss 0.0780 | Train Acc 0.9784| Test Acc 0.9455\n",
      "Epoch 00063 | Loss 0.0765 | Train Acc 0.9799| Test Acc 0.9455\n",
      "Epoch 00064 | Loss 0.0750 | Train Acc 0.9813| Test Acc 0.9455\n",
      "Epoch 00065 | Loss 0.0736 | Train Acc 0.9821| Test Acc 0.9455\n",
      "Epoch 00066 | Loss 0.0722 | Train Acc 0.9821| Test Acc 0.9455\n",
      "Epoch 00067 | Loss 0.0708 | Train Acc 0.9821| Test Acc 0.9470\n",
      "Epoch 00068 | Loss 0.0694 | Train Acc 0.9828| Test Acc 0.9470\n",
      "Epoch 00069 | Loss 0.0681 | Train Acc 0.9828| Test Acc 0.9470\n",
      "Epoch 00070 | Loss 0.0667 | Train Acc 0.9836| Test Acc 0.9470\n",
      "Epoch 00071 | Loss 0.0654 | Train Acc 0.9836| Test Acc 0.9470\n",
      "Epoch 00072 | Loss 0.0642 | Train Acc 0.9851| Test Acc 0.9470\n",
      "Epoch 00073 | Loss 0.0629 | Train Acc 0.9851| Test Acc 0.9470\n",
      "Epoch 00074 | Loss 0.0617 | Train Acc 0.9851| Test Acc 0.9500\n",
      "Epoch 00075 | Loss 0.0605 | Train Acc 0.9851| Test Acc 0.9515\n",
      "Epoch 00076 | Loss 0.0593 | Train Acc 0.9866| Test Acc 0.9530\n",
      "Epoch 00077 | Loss 0.0581 | Train Acc 0.9881| Test Acc 0.9530\n",
      "Epoch 00078 | Loss 0.0570 | Train Acc 0.9896| Test Acc 0.9530\n",
      "Epoch 00079 | Loss 0.0558 | Train Acc 0.9896| Test Acc 0.9530\n",
      "Epoch 00080 | Loss 0.0547 | Train Acc 0.9896| Test Acc 0.9561\n",
      "Epoch 00081 | Loss 0.0536 | Train Acc 0.9903| Test Acc 0.9561\n",
      "Epoch 00082 | Loss 0.0526 | Train Acc 0.9903| Test Acc 0.9576\n",
      "Epoch 00083 | Loss 0.0515 | Train Acc 0.9903| Test Acc 0.9576\n",
      "Epoch 00084 | Loss 0.0505 | Train Acc 0.9903| Test Acc 0.9591\n",
      "Epoch 00085 | Loss 0.0495 | Train Acc 0.9903| Test Acc 0.9591\n",
      "Epoch 00086 | Loss 0.0485 | Train Acc 0.9903| Test Acc 0.9606\n",
      "Epoch 00087 | Loss 0.0475 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00088 | Loss 0.0466 | Train Acc 0.9918| Test Acc 0.9621\n",
      "Epoch 00089 | Loss 0.0456 | Train Acc 0.9925| Test Acc 0.9621\n",
      "Epoch 00090 | Loss 0.0447 | Train Acc 0.9933| Test Acc 0.9621\n",
      "Epoch 00091 | Loss 0.0438 | Train Acc 0.9940| Test Acc 0.9636\n",
      "Epoch 00092 | Loss 0.0429 | Train Acc 0.9940| Test Acc 0.9636\n",
      "Epoch 00093 | Loss 0.0421 | Train Acc 0.9940| Test Acc 0.9636\n",
      "Epoch 00094 | Loss 0.0413 | Train Acc 0.9948| Test Acc 0.9636\n",
      "Epoch 00095 | Loss 0.0404 | Train Acc 0.9948| Test Acc 0.9606\n",
      "Epoch 00096 | Loss 0.0396 | Train Acc 0.9948| Test Acc 0.9606\n",
      "Epoch 00097 | Loss 0.0389 | Train Acc 0.9955| Test Acc 0.9606\n",
      "Epoch 00098 | Loss 0.0381 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00099 | Loss 0.0373 | Train Acc 0.9963| Test Acc 0.9621\n",
      "Epoch 00100 | Loss 0.0366 | Train Acc 0.9963| Test Acc 0.9621\n",
      "Epoch 00101 | Loss 0.0359 | Train Acc 0.9963| Test Acc 0.9621\n",
      "Epoch 00102 | Loss 0.0352 | Train Acc 0.9963| Test Acc 0.9636\n",
      "Epoch 00103 | Loss 0.0345 | Train Acc 0.9963| Test Acc 0.9636\n",
      "Epoch 00104 | Loss 0.0338 | Train Acc 0.9970| Test Acc 0.9636\n",
      "Epoch 00105 | Loss 0.0332 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00106 | Loss 0.0325 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00107 | Loss 0.0319 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00108 | Loss 0.0313 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00109 | Loss 0.0306 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00110 | Loss 0.0300 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00111 | Loss 0.0295 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00112 | Loss 0.0289 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00113 | Loss 0.0283 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00114 | Loss 0.0278 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00115 | Loss 0.0272 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00116 | Loss 0.0267 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00117 | Loss 0.0262 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00118 | Loss 0.0257 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00119 | Loss 0.0252 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00120 | Loss 0.0247 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00121 | Loss 0.0242 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00122 | Loss 0.0237 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00123 | Loss 0.0233 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00124 | Loss 0.0228 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00125 | Loss 0.0224 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00126 | Loss 0.0219 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00127 | Loss 0.0215 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00128 | Loss 0.0211 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00129 | Loss 0.0207 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00130 | Loss 0.0203 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00131 | Loss 0.0199 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00132 | Loss 0.0195 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00133 | Loss 0.0191 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00134 | Loss 0.0188 | Train Acc 0.9985| Test Acc 0.9697\n",
      "Epoch 00135 | Loss 0.0184 | Train Acc 0.9985| Test Acc 0.9697\n",
      "Epoch 00136 | Loss 0.0181 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00137 | Loss 0.0177 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00138 | Loss 0.0174 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00139 | Loss 0.0171 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00140 | Loss 0.0167 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00141 | Loss 0.0164 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00142 | Loss 0.0161 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00143 | Loss 0.0158 | Train Acc 0.9993| Test Acc 0.9682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00144 | Loss 0.0155 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00145 | Loss 0.0152 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00146 | Loss 0.0149 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00147 | Loss 0.0146 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00148 | Loss 0.0143 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00149 | Loss 0.0141 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00150 | Loss 0.0138 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00151 | Loss 0.0136 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00152 | Loss 0.0133 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00153 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00154 | Loss 0.0128 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00155 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00156 | Loss 0.0124 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00157 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00158 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00159 | Loss 0.0117 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00160 | Loss 0.0115 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00161 | Loss 0.0113 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00162 | Loss 0.0111 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00163 | Loss 0.0109 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00164 | Loss 0.0107 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00165 | Loss 0.0105 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00166 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00167 | Loss 0.0102 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00168 | Loss 0.0100 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00169 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00170 | Loss 0.0097 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00171 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00172 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00173 | Loss 0.0092 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00174 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00175 | Loss 0.0089 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00176 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00177 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00178 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00179 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00180 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00181 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00182 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00183 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00184 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00185 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00186 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00187 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00188 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00189 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00190 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00191 | Loss 0.0070 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00192 | Loss 0.0069 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00193 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00194 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00195 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00196 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00197 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00198 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00199 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00000 | Loss 0.7801 | Train Acc 0.5836| Test Acc 0.5788\n",
      "Epoch 00001 | Loss 0.6631 | Train Acc 0.6269| Test Acc 0.6348\n",
      "Epoch 00002 | Loss 0.5977 | Train Acc 0.6776| Test Acc 0.6818\n",
      "Epoch 00003 | Loss 0.5428 | Train Acc 0.7149| Test Acc 0.7227\n",
      "Epoch 00004 | Loss 0.4954 | Train Acc 0.7522| Test Acc 0.7591\n",
      "Epoch 00005 | Loss 0.4544 | Train Acc 0.7806| Test Acc 0.7879\n",
      "Epoch 00006 | Loss 0.4192 | Train Acc 0.8022| Test Acc 0.8030\n",
      "Epoch 00007 | Loss 0.3890 | Train Acc 0.8239| Test Acc 0.8212\n",
      "Epoch 00008 | Loss 0.3630 | Train Acc 0.8381| Test Acc 0.8333\n",
      "Epoch 00009 | Loss 0.3405 | Train Acc 0.8515| Test Acc 0.8409\n",
      "Epoch 00010 | Loss 0.3205 | Train Acc 0.8701| Test Acc 0.8545\n",
      "Epoch 00011 | Loss 0.3026 | Train Acc 0.8799| Test Acc 0.8621\n",
      "Epoch 00012 | Loss 0.2864 | Train Acc 0.8903| Test Acc 0.8697\n",
      "Epoch 00013 | Loss 0.2717 | Train Acc 0.9000| Test Acc 0.8803\n",
      "Epoch 00014 | Loss 0.2582 | Train Acc 0.9082| Test Acc 0.8894\n",
      "Epoch 00015 | Loss 0.2460 | Train Acc 0.9142| Test Acc 0.8939\n",
      "Epoch 00016 | Loss 0.2348 | Train Acc 0.9164| Test Acc 0.9030\n",
      "Epoch 00017 | Loss 0.2245 | Train Acc 0.9201| Test Acc 0.9121\n",
      "Epoch 00018 | Loss 0.2150 | Train Acc 0.9246| Test Acc 0.9152\n",
      "Epoch 00019 | Loss 0.2061 | Train Acc 0.9291| Test Acc 0.9182\n",
      "Epoch 00020 | Loss 0.1978 | Train Acc 0.9313| Test Acc 0.9182\n",
      "Epoch 00021 | Loss 0.1899 | Train Acc 0.9366| Test Acc 0.9182\n",
      "Epoch 00022 | Loss 0.1825 | Train Acc 0.9403| Test Acc 0.9258\n",
      "Epoch 00023 | Loss 0.1756 | Train Acc 0.9455| Test Acc 0.9288\n",
      "Epoch 00024 | Loss 0.1691 | Train Acc 0.9500| Test Acc 0.9318\n",
      "Epoch 00025 | Loss 0.1629 | Train Acc 0.9530| Test Acc 0.9333\n",
      "Epoch 00026 | Loss 0.1571 | Train Acc 0.9560| Test Acc 0.9348\n",
      "Epoch 00027 | Loss 0.1516 | Train Acc 0.9582| Test Acc 0.9394\n",
      "Epoch 00028 | Loss 0.1464 | Train Acc 0.9604| Test Acc 0.9439\n",
      "Epoch 00029 | Loss 0.1414 | Train Acc 0.9612| Test Acc 0.9424\n",
      "Epoch 00030 | Loss 0.1367 | Train Acc 0.9619| Test Acc 0.9455\n",
      "Epoch 00031 | Loss 0.1322 | Train Acc 0.9627| Test Acc 0.9439\n",
      "Epoch 00032 | Loss 0.1280 | Train Acc 0.9657| Test Acc 0.9455\n",
      "Epoch 00033 | Loss 0.1240 | Train Acc 0.9664| Test Acc 0.9455\n",
      "Epoch 00034 | Loss 0.1202 | Train Acc 0.9679| Test Acc 0.9455\n",
      "Epoch 00035 | Loss 0.1165 | Train Acc 0.9687| Test Acc 0.9455\n",
      "Epoch 00036 | Loss 0.1130 | Train Acc 0.9687| Test Acc 0.9455\n",
      "Epoch 00037 | Loss 0.1096 | Train Acc 0.9701| Test Acc 0.9455\n",
      "Epoch 00038 | Loss 0.1063 | Train Acc 0.9716| Test Acc 0.9455\n",
      "Epoch 00039 | Loss 0.1031 | Train Acc 0.9716| Test Acc 0.9455\n",
      "Epoch 00040 | Loss 0.1000 | Train Acc 0.9724| Test Acc 0.9455\n",
      "Epoch 00041 | Loss 0.0971 | Train Acc 0.9724| Test Acc 0.9455\n",
      "Epoch 00042 | Loss 0.0942 | Train Acc 0.9731| Test Acc 0.9455\n",
      "Epoch 00043 | Loss 0.0915 | Train Acc 0.9739| Test Acc 0.9455\n",
      "Epoch 00044 | Loss 0.0889 | Train Acc 0.9754| Test Acc 0.9470\n",
      "Epoch 00045 | Loss 0.0864 | Train Acc 0.9761| Test Acc 0.9470\n",
      "Epoch 00046 | Loss 0.0839 | Train Acc 0.9761| Test Acc 0.9455\n",
      "Epoch 00047 | Loss 0.0816 | Train Acc 0.9769| Test Acc 0.9455\n",
      "Epoch 00048 | Loss 0.0794 | Train Acc 0.9784| Test Acc 0.9485\n",
      "Epoch 00049 | Loss 0.0773 | Train Acc 0.9799| Test Acc 0.9485\n",
      "Epoch 00050 | Loss 0.0752 | Train Acc 0.9806| Test Acc 0.9485\n",
      "Epoch 00051 | Loss 0.0732 | Train Acc 0.9821| Test Acc 0.9485\n",
      "Epoch 00052 | Loss 0.0713 | Train Acc 0.9828| Test Acc 0.9485\n",
      "Epoch 00053 | Loss 0.0694 | Train Acc 0.9828| Test Acc 0.9515\n",
      "Epoch 00054 | Loss 0.0676 | Train Acc 0.9836| Test Acc 0.9545\n",
      "Epoch 00055 | Loss 0.0658 | Train Acc 0.9843| Test Acc 0.9545\n",
      "Epoch 00056 | Loss 0.0641 | Train Acc 0.9851| Test Acc 0.9545\n",
      "Epoch 00057 | Loss 0.0624 | Train Acc 0.9851| Test Acc 0.9545\n",
      "Epoch 00058 | Loss 0.0608 | Train Acc 0.9866| Test Acc 0.9545\n",
      "Epoch 00059 | Loss 0.0592 | Train Acc 0.9866| Test Acc 0.9545\n",
      "Epoch 00060 | Loss 0.0577 | Train Acc 0.9873| Test Acc 0.9545\n",
      "Epoch 00061 | Loss 0.0562 | Train Acc 0.9873| Test Acc 0.9545\n",
      "Epoch 00062 | Loss 0.0548 | Train Acc 0.9873| Test Acc 0.9530\n",
      "Epoch 00063 | Loss 0.0534 | Train Acc 0.9881| Test Acc 0.9530\n",
      "Epoch 00064 | Loss 0.0520 | Train Acc 0.9881| Test Acc 0.9545\n",
      "Epoch 00065 | Loss 0.0507 | Train Acc 0.9896| Test Acc 0.9545\n",
      "Epoch 00066 | Loss 0.0494 | Train Acc 0.9903| Test Acc 0.9545\n",
      "Epoch 00067 | Loss 0.0482 | Train Acc 0.9903| Test Acc 0.9545\n",
      "Epoch 00068 | Loss 0.0469 | Train Acc 0.9925| Test Acc 0.9530\n",
      "Epoch 00069 | Loss 0.0457 | Train Acc 0.9933| Test Acc 0.9530\n",
      "Epoch 00070 | Loss 0.0445 | Train Acc 0.9933| Test Acc 0.9545\n",
      "Epoch 00071 | Loss 0.0434 | Train Acc 0.9940| Test Acc 0.9545\n",
      "Epoch 00072 | Loss 0.0423 | Train Acc 0.9948| Test Acc 0.9545\n",
      "Epoch 00073 | Loss 0.0412 | Train Acc 0.9948| Test Acc 0.9545\n",
      "Epoch 00074 | Loss 0.0401 | Train Acc 0.9955| Test Acc 0.9545\n",
      "Epoch 00075 | Loss 0.0391 | Train Acc 0.9955| Test Acc 0.9545\n",
      "Epoch 00076 | Loss 0.0381 | Train Acc 0.9955| Test Acc 0.9545\n",
      "Epoch 00077 | Loss 0.0371 | Train Acc 0.9955| Test Acc 0.9545\n",
      "Epoch 00078 | Loss 0.0361 | Train Acc 0.9963| Test Acc 0.9545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00079 | Loss 0.0352 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00080 | Loss 0.0343 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00081 | Loss 0.0334 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00082 | Loss 0.0325 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00083 | Loss 0.0317 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00084 | Loss 0.0309 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00085 | Loss 0.0301 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00086 | Loss 0.0293 | Train Acc 0.9963| Test Acc 0.9530\n",
      "Epoch 00087 | Loss 0.0286 | Train Acc 0.9963| Test Acc 0.9545\n",
      "Epoch 00088 | Loss 0.0279 | Train Acc 0.9963| Test Acc 0.9561\n",
      "Epoch 00089 | Loss 0.0271 | Train Acc 0.9963| Test Acc 0.9561\n",
      "Epoch 00090 | Loss 0.0265 | Train Acc 0.9963| Test Acc 0.9561\n",
      "Epoch 00091 | Loss 0.0258 | Train Acc 0.9963| Test Acc 0.9576\n",
      "Epoch 00092 | Loss 0.0252 | Train Acc 0.9963| Test Acc 0.9576\n",
      "Epoch 00093 | Loss 0.0245 | Train Acc 0.9963| Test Acc 0.9576\n",
      "Epoch 00094 | Loss 0.0239 | Train Acc 0.9963| Test Acc 0.9576\n",
      "Epoch 00095 | Loss 0.0233 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00096 | Loss 0.0227 | Train Acc 0.9970| Test Acc 0.9606\n",
      "Epoch 00097 | Loss 0.0221 | Train Acc 0.9970| Test Acc 0.9606\n",
      "Epoch 00098 | Loss 0.0216 | Train Acc 0.9970| Test Acc 0.9606\n",
      "Epoch 00099 | Loss 0.0210 | Train Acc 0.9970| Test Acc 0.9591\n",
      "Epoch 00100 | Loss 0.0205 | Train Acc 0.9978| Test Acc 0.9606\n",
      "Epoch 00101 | Loss 0.0200 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00102 | Loss 0.0195 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00103 | Loss 0.0191 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00104 | Loss 0.0186 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00105 | Loss 0.0181 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00106 | Loss 0.0177 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00107 | Loss 0.0173 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00108 | Loss 0.0169 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00109 | Loss 0.0165 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00110 | Loss 0.0161 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00111 | Loss 0.0157 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00112 | Loss 0.0153 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00113 | Loss 0.0149 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00114 | Loss 0.0146 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00115 | Loss 0.0143 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00116 | Loss 0.0139 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00117 | Loss 0.0136 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00118 | Loss 0.0133 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00119 | Loss 0.0130 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00120 | Loss 0.0127 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00121 | Loss 0.0124 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00122 | Loss 0.0121 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00123 | Loss 0.0119 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00124 | Loss 0.0116 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00125 | Loss 0.0114 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00126 | Loss 0.0111 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00127 | Loss 0.0109 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00128 | Loss 0.0106 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00129 | Loss 0.0104 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00130 | Loss 0.0102 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00131 | Loss 0.0100 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00132 | Loss 0.0097 | Train Acc 0.9993| Test Acc 0.9636\n",
      "Epoch 00133 | Loss 0.0095 | Train Acc 0.9993| Test Acc 0.9636\n",
      "Epoch 00134 | Loss 0.0093 | Train Acc 0.9993| Test Acc 0.9636\n",
      "Epoch 00135 | Loss 0.0091 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00136 | Loss 0.0090 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00137 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00138 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00139 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00140 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00141 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00142 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00143 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00144 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00145 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00146 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00147 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00148 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00149 | Loss 0.0069 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00150 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00151 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00152 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00153 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00154 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00155 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00156 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00157 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00158 | Loss 0.0059 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00159 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00160 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00161 | Loss 0.0056 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00162 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00163 | Loss 0.0054 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00164 | Loss 0.0053 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00165 | Loss 0.0052 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00166 | Loss 0.0051 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00167 | Loss 0.0050 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00168 | Loss 0.0050 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00169 | Loss 0.0049 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00170 | Loss 0.0048 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00171 | Loss 0.0047 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00172 | Loss 0.0047 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00173 | Loss 0.0046 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00174 | Loss 0.0045 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00175 | Loss 0.0044 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00176 | Loss 0.0044 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00177 | Loss 0.0043 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00178 | Loss 0.0042 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00179 | Loss 0.0042 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00180 | Loss 0.0041 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00181 | Loss 0.0041 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00182 | Loss 0.0040 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00183 | Loss 0.0039 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00184 | Loss 0.0039 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00185 | Loss 0.0038 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00186 | Loss 0.0038 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00187 | Loss 0.0037 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00188 | Loss 0.0037 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00189 | Loss 0.0036 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00190 | Loss 0.0036 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00191 | Loss 0.0035 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00192 | Loss 0.0035 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00193 | Loss 0.0034 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00194 | Loss 0.0034 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00195 | Loss 0.0033 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00196 | Loss 0.0033 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00197 | Loss 0.0032 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00198 | Loss 0.0032 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00199 | Loss 0.0032 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00000 | Loss 1.0748 | Train Acc 0.5612| Test Acc 0.5364\n",
      "Epoch 00001 | Loss 0.8886 | Train Acc 0.6134| Test Acc 0.5833\n",
      "Epoch 00002 | Loss 0.7556 | Train Acc 0.6604| Test Acc 0.6227\n",
      "Epoch 00003 | Loss 0.6477 | Train Acc 0.7187| Test Acc 0.6652\n",
      "Epoch 00004 | Loss 0.5628 | Train Acc 0.7560| Test Acc 0.7106\n",
      "Epoch 00005 | Loss 0.4974 | Train Acc 0.7799| Test Acc 0.7500\n",
      "Epoch 00006 | Loss 0.4473 | Train Acc 0.8030| Test Acc 0.7833\n",
      "Epoch 00007 | Loss 0.4087 | Train Acc 0.8261| Test Acc 0.8076\n",
      "Epoch 00008 | Loss 0.3784 | Train Acc 0.8425| Test Acc 0.8273\n",
      "Epoch 00009 | Loss 0.3534 | Train Acc 0.8575| Test Acc 0.8333\n",
      "Epoch 00010 | Loss 0.3319 | Train Acc 0.8627| Test Acc 0.8470\n",
      "Epoch 00011 | Loss 0.3129 | Train Acc 0.8776| Test Acc 0.8561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012 | Loss 0.2956 | Train Acc 0.8836| Test Acc 0.8682\n",
      "Epoch 00013 | Loss 0.2799 | Train Acc 0.8896| Test Acc 0.8758\n",
      "Epoch 00014 | Loss 0.2657 | Train Acc 0.8955| Test Acc 0.8833\n",
      "Epoch 00015 | Loss 0.2529 | Train Acc 0.8993| Test Acc 0.8848\n",
      "Epoch 00016 | Loss 0.2414 | Train Acc 0.9022| Test Acc 0.8864\n",
      "Epoch 00017 | Loss 0.2311 | Train Acc 0.9104| Test Acc 0.8879\n",
      "Epoch 00018 | Loss 0.2218 | Train Acc 0.9142| Test Acc 0.8939\n",
      "Epoch 00019 | Loss 0.2133 | Train Acc 0.9172| Test Acc 0.8939\n",
      "Epoch 00020 | Loss 0.2057 | Train Acc 0.9201| Test Acc 0.8924\n",
      "Epoch 00021 | Loss 0.1988 | Train Acc 0.9254| Test Acc 0.8955\n",
      "Epoch 00022 | Loss 0.1926 | Train Acc 0.9284| Test Acc 0.8939\n",
      "Epoch 00023 | Loss 0.1869 | Train Acc 0.9291| Test Acc 0.8970\n",
      "Epoch 00024 | Loss 0.1817 | Train Acc 0.9336| Test Acc 0.9030\n",
      "Epoch 00025 | Loss 0.1769 | Train Acc 0.9351| Test Acc 0.9015\n",
      "Epoch 00026 | Loss 0.1723 | Train Acc 0.9373| Test Acc 0.9045\n",
      "Epoch 00027 | Loss 0.1680 | Train Acc 0.9403| Test Acc 0.9061\n",
      "Epoch 00028 | Loss 0.1639 | Train Acc 0.9410| Test Acc 0.9076\n",
      "Epoch 00029 | Loss 0.1600 | Train Acc 0.9433| Test Acc 0.9091\n",
      "Epoch 00030 | Loss 0.1563 | Train Acc 0.9433| Test Acc 0.9136\n",
      "Epoch 00031 | Loss 0.1526 | Train Acc 0.9478| Test Acc 0.9167\n",
      "Epoch 00032 | Loss 0.1491 | Train Acc 0.9485| Test Acc 0.9167\n",
      "Epoch 00033 | Loss 0.1456 | Train Acc 0.9463| Test Acc 0.9167\n",
      "Epoch 00034 | Loss 0.1423 | Train Acc 0.9478| Test Acc 0.9197\n",
      "Epoch 00035 | Loss 0.1391 | Train Acc 0.9500| Test Acc 0.9212\n",
      "Epoch 00036 | Loss 0.1360 | Train Acc 0.9530| Test Acc 0.9227\n",
      "Epoch 00037 | Loss 0.1330 | Train Acc 0.9552| Test Acc 0.9227\n",
      "Epoch 00038 | Loss 0.1301 | Train Acc 0.9590| Test Acc 0.9258\n",
      "Epoch 00039 | Loss 0.1273 | Train Acc 0.9604| Test Acc 0.9273\n",
      "Epoch 00040 | Loss 0.1246 | Train Acc 0.9619| Test Acc 0.9258\n",
      "Epoch 00041 | Loss 0.1220 | Train Acc 0.9634| Test Acc 0.9242\n",
      "Epoch 00042 | Loss 0.1195 | Train Acc 0.9642| Test Acc 0.9258\n",
      "Epoch 00043 | Loss 0.1171 | Train Acc 0.9634| Test Acc 0.9273\n",
      "Epoch 00044 | Loss 0.1148 | Train Acc 0.9634| Test Acc 0.9273\n",
      "Epoch 00045 | Loss 0.1126 | Train Acc 0.9642| Test Acc 0.9273\n",
      "Epoch 00046 | Loss 0.1105 | Train Acc 0.9642| Test Acc 0.9288\n",
      "Epoch 00047 | Loss 0.1085 | Train Acc 0.9657| Test Acc 0.9288\n",
      "Epoch 00048 | Loss 0.1065 | Train Acc 0.9672| Test Acc 0.9303\n",
      "Epoch 00049 | Loss 0.1046 | Train Acc 0.9672| Test Acc 0.9318\n",
      "Epoch 00050 | Loss 0.1027 | Train Acc 0.9679| Test Acc 0.9348\n",
      "Epoch 00051 | Loss 0.1009 | Train Acc 0.9701| Test Acc 0.9348\n",
      "Epoch 00052 | Loss 0.0991 | Train Acc 0.9694| Test Acc 0.9348\n",
      "Epoch 00053 | Loss 0.0974 | Train Acc 0.9694| Test Acc 0.9364\n",
      "Epoch 00054 | Loss 0.0957 | Train Acc 0.9709| Test Acc 0.9364\n",
      "Epoch 00055 | Loss 0.0940 | Train Acc 0.9709| Test Acc 0.9364\n",
      "Epoch 00056 | Loss 0.0924 | Train Acc 0.9716| Test Acc 0.9364\n",
      "Epoch 00057 | Loss 0.0908 | Train Acc 0.9716| Test Acc 0.9348\n",
      "Epoch 00058 | Loss 0.0892 | Train Acc 0.9731| Test Acc 0.9364\n",
      "Epoch 00059 | Loss 0.0876 | Train Acc 0.9731| Test Acc 0.9364\n",
      "Epoch 00060 | Loss 0.0861 | Train Acc 0.9739| Test Acc 0.9364\n",
      "Epoch 00061 | Loss 0.0846 | Train Acc 0.9754| Test Acc 0.9364\n",
      "Epoch 00062 | Loss 0.0832 | Train Acc 0.9769| Test Acc 0.9364\n",
      "Epoch 00063 | Loss 0.0818 | Train Acc 0.9769| Test Acc 0.9379\n",
      "Epoch 00064 | Loss 0.0804 | Train Acc 0.9776| Test Acc 0.9379\n",
      "Epoch 00065 | Loss 0.0790 | Train Acc 0.9776| Test Acc 0.9394\n",
      "Epoch 00066 | Loss 0.0777 | Train Acc 0.9784| Test Acc 0.9394\n",
      "Epoch 00067 | Loss 0.0764 | Train Acc 0.9784| Test Acc 0.9394\n",
      "Epoch 00068 | Loss 0.0752 | Train Acc 0.9784| Test Acc 0.9394\n",
      "Epoch 00069 | Loss 0.0739 | Train Acc 0.9791| Test Acc 0.9394\n",
      "Epoch 00070 | Loss 0.0727 | Train Acc 0.9799| Test Acc 0.9394\n",
      "Epoch 00071 | Loss 0.0714 | Train Acc 0.9799| Test Acc 0.9409\n",
      "Epoch 00072 | Loss 0.0702 | Train Acc 0.9813| Test Acc 0.9409\n",
      "Epoch 00073 | Loss 0.0690 | Train Acc 0.9813| Test Acc 0.9409\n",
      "Epoch 00074 | Loss 0.0679 | Train Acc 0.9813| Test Acc 0.9409\n",
      "Epoch 00075 | Loss 0.0667 | Train Acc 0.9828| Test Acc 0.9409\n",
      "Epoch 00076 | Loss 0.0656 | Train Acc 0.9836| Test Acc 0.9424\n",
      "Epoch 00077 | Loss 0.0644 | Train Acc 0.9836| Test Acc 0.9424\n",
      "Epoch 00078 | Loss 0.0633 | Train Acc 0.9836| Test Acc 0.9424\n",
      "Epoch 00079 | Loss 0.0622 | Train Acc 0.9851| Test Acc 0.9424\n",
      "Epoch 00080 | Loss 0.0611 | Train Acc 0.9858| Test Acc 0.9424\n",
      "Epoch 00081 | Loss 0.0601 | Train Acc 0.9858| Test Acc 0.9439\n",
      "Epoch 00082 | Loss 0.0591 | Train Acc 0.9858| Test Acc 0.9455\n",
      "Epoch 00083 | Loss 0.0580 | Train Acc 0.9858| Test Acc 0.9470\n",
      "Epoch 00084 | Loss 0.0570 | Train Acc 0.9858| Test Acc 0.9470\n",
      "Epoch 00085 | Loss 0.0561 | Train Acc 0.9873| Test Acc 0.9470\n",
      "Epoch 00086 | Loss 0.0551 | Train Acc 0.9881| Test Acc 0.9470\n",
      "Epoch 00087 | Loss 0.0541 | Train Acc 0.9881| Test Acc 0.9470\n",
      "Epoch 00088 | Loss 0.0532 | Train Acc 0.9888| Test Acc 0.9485\n",
      "Epoch 00089 | Loss 0.0523 | Train Acc 0.9888| Test Acc 0.9485\n",
      "Epoch 00090 | Loss 0.0514 | Train Acc 0.9888| Test Acc 0.9485\n",
      "Epoch 00091 | Loss 0.0505 | Train Acc 0.9896| Test Acc 0.9485\n",
      "Epoch 00092 | Loss 0.0496 | Train Acc 0.9896| Test Acc 0.9485\n",
      "Epoch 00093 | Loss 0.0487 | Train Acc 0.9896| Test Acc 0.9485\n",
      "Epoch 00094 | Loss 0.0479 | Train Acc 0.9896| Test Acc 0.9500\n",
      "Epoch 00095 | Loss 0.0471 | Train Acc 0.9903| Test Acc 0.9500\n",
      "Epoch 00096 | Loss 0.0462 | Train Acc 0.9910| Test Acc 0.9500\n",
      "Epoch 00097 | Loss 0.0454 | Train Acc 0.9918| Test Acc 0.9500\n",
      "Epoch 00098 | Loss 0.0446 | Train Acc 0.9925| Test Acc 0.9500\n",
      "Epoch 00099 | Loss 0.0439 | Train Acc 0.9933| Test Acc 0.9500\n",
      "Epoch 00100 | Loss 0.0431 | Train Acc 0.9933| Test Acc 0.9500\n",
      "Epoch 00101 | Loss 0.0424 | Train Acc 0.9940| Test Acc 0.9500\n",
      "Epoch 00102 | Loss 0.0416 | Train Acc 0.9940| Test Acc 0.9500\n",
      "Epoch 00103 | Loss 0.0409 | Train Acc 0.9940| Test Acc 0.9500\n",
      "Epoch 00104 | Loss 0.0402 | Train Acc 0.9948| Test Acc 0.9500\n",
      "Epoch 00105 | Loss 0.0395 | Train Acc 0.9948| Test Acc 0.9500\n",
      "Epoch 00106 | Loss 0.0388 | Train Acc 0.9948| Test Acc 0.9500\n",
      "Epoch 00107 | Loss 0.0381 | Train Acc 0.9955| Test Acc 0.9500\n",
      "Epoch 00108 | Loss 0.0375 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00109 | Loss 0.0368 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00110 | Loss 0.0362 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00111 | Loss 0.0355 | Train Acc 0.9955| Test Acc 0.9515\n",
      "Epoch 00112 | Loss 0.0349 | Train Acc 0.9963| Test Acc 0.9515\n",
      "Epoch 00113 | Loss 0.0343 | Train Acc 0.9963| Test Acc 0.9515\n",
      "Epoch 00114 | Loss 0.0337 | Train Acc 0.9970| Test Acc 0.9515\n",
      "Epoch 00115 | Loss 0.0331 | Train Acc 0.9970| Test Acc 0.9515\n",
      "Epoch 00116 | Loss 0.0325 | Train Acc 0.9970| Test Acc 0.9515\n",
      "Epoch 00117 | Loss 0.0319 | Train Acc 0.9970| Test Acc 0.9530\n",
      "Epoch 00118 | Loss 0.0314 | Train Acc 0.9970| Test Acc 0.9545\n",
      "Epoch 00119 | Loss 0.0308 | Train Acc 0.9970| Test Acc 0.9545\n",
      "Epoch 00120 | Loss 0.0302 | Train Acc 0.9970| Test Acc 0.9545\n",
      "Epoch 00121 | Loss 0.0297 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00122 | Loss 0.0292 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00123 | Loss 0.0286 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00124 | Loss 0.0281 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00125 | Loss 0.0276 | Train Acc 0.9970| Test Acc 0.9561\n",
      "Epoch 00126 | Loss 0.0271 | Train Acc 0.9978| Test Acc 0.9561\n",
      "Epoch 00127 | Loss 0.0266 | Train Acc 0.9978| Test Acc 0.9561\n",
      "Epoch 00128 | Loss 0.0261 | Train Acc 0.9978| Test Acc 0.9561\n",
      "Epoch 00129 | Loss 0.0256 | Train Acc 0.9978| Test Acc 0.9561\n",
      "Epoch 00130 | Loss 0.0252 | Train Acc 0.9978| Test Acc 0.9576\n",
      "Epoch 00131 | Loss 0.0247 | Train Acc 0.9978| Test Acc 0.9576\n",
      "Epoch 00132 | Loss 0.0242 | Train Acc 0.9978| Test Acc 0.9576\n",
      "Epoch 00133 | Loss 0.0238 | Train Acc 0.9978| Test Acc 0.9576\n",
      "Epoch 00134 | Loss 0.0234 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00135 | Loss 0.0229 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00136 | Loss 0.0225 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00137 | Loss 0.0221 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00138 | Loss 0.0217 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00139 | Loss 0.0213 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00140 | Loss 0.0209 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00141 | Loss 0.0205 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00142 | Loss 0.0201 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00143 | Loss 0.0198 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00144 | Loss 0.0194 | Train Acc 0.9985| Test Acc 0.9576\n",
      "Epoch 00145 | Loss 0.0190 | Train Acc 0.9985| Test Acc 0.9591\n",
      "Epoch 00146 | Loss 0.0187 | Train Acc 0.9985| Test Acc 0.9591\n",
      "Epoch 00147 | Loss 0.0183 | Train Acc 0.9985| Test Acc 0.9591\n",
      "Epoch 00148 | Loss 0.0180 | Train Acc 0.9985| Test Acc 0.9591\n",
      "Epoch 00149 | Loss 0.0177 | Train Acc 0.9985| Test Acc 0.9591\n",
      "Epoch 00150 | Loss 0.0174 | Train Acc 0.9993| Test Acc 0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00151 | Loss 0.0170 | Train Acc 0.9993| Test Acc 0.9591\n",
      "Epoch 00152 | Loss 0.0167 | Train Acc 0.9993| Test Acc 0.9591\n",
      "Epoch 00153 | Loss 0.0164 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00154 | Loss 0.0161 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00155 | Loss 0.0158 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00156 | Loss 0.0155 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00157 | Loss 0.0152 | Train Acc 0.9993| Test Acc 0.9621\n",
      "Epoch 00158 | Loss 0.0150 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00159 | Loss 0.0147 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00160 | Loss 0.0144 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00161 | Loss 0.0142 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00162 | Loss 0.0139 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00163 | Loss 0.0137 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00164 | Loss 0.0134 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00165 | Loss 0.0132 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00166 | Loss 0.0130 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00167 | Loss 0.0128 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00168 | Loss 0.0125 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00169 | Loss 0.0123 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00170 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00171 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00172 | Loss 0.0117 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00173 | Loss 0.0115 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00174 | Loss 0.0113 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00175 | Loss 0.0111 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00176 | Loss 0.0109 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00177 | Loss 0.0107 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00178 | Loss 0.0106 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00179 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00180 | Loss 0.0102 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00181 | Loss 0.0100 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00182 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00183 | Loss 0.0097 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00184 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00185 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00186 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00187 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00188 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00189 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00190 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00191 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00192 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00193 | Loss 0.0083 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00194 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00195 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00196 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00197 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00198 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00199 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9667\n",
      "Epoch 00000 | Loss 1.0136 | Train Acc 0.4821| Test Acc 0.4379\n",
      "Epoch 00001 | Loss 0.8499 | Train Acc 0.5552| Test Acc 0.5182\n",
      "Epoch 00002 | Loss 0.7363 | Train Acc 0.6254| Test Acc 0.5924\n",
      "Epoch 00003 | Loss 0.6426 | Train Acc 0.6843| Test Acc 0.6561\n",
      "Epoch 00004 | Loss 0.5667 | Train Acc 0.7440| Test Acc 0.7152\n",
      "Epoch 00005 | Loss 0.5057 | Train Acc 0.7791| Test Acc 0.7530\n",
      "Epoch 00006 | Loss 0.4564 | Train Acc 0.7993| Test Acc 0.7803\n",
      "Epoch 00007 | Loss 0.4160 | Train Acc 0.8194| Test Acc 0.7955\n",
      "Epoch 00008 | Loss 0.3824 | Train Acc 0.8381| Test Acc 0.8167\n",
      "Epoch 00009 | Loss 0.3541 | Train Acc 0.8515| Test Acc 0.8273\n",
      "Epoch 00010 | Loss 0.3298 | Train Acc 0.8619| Test Acc 0.8409\n",
      "Epoch 00011 | Loss 0.3087 | Train Acc 0.8731| Test Acc 0.8439\n",
      "Epoch 00012 | Loss 0.2901 | Train Acc 0.8828| Test Acc 0.8561\n",
      "Epoch 00013 | Loss 0.2736 | Train Acc 0.8910| Test Acc 0.8667\n",
      "Epoch 00014 | Loss 0.2589 | Train Acc 0.8985| Test Acc 0.8697\n",
      "Epoch 00015 | Loss 0.2456 | Train Acc 0.9075| Test Acc 0.8758\n",
      "Epoch 00016 | Loss 0.2335 | Train Acc 0.9194| Test Acc 0.8848\n",
      "Epoch 00017 | Loss 0.2226 | Train Acc 0.9239| Test Acc 0.8879\n",
      "Epoch 00018 | Loss 0.2126 | Train Acc 0.9284| Test Acc 0.8894\n",
      "Epoch 00019 | Loss 0.2034 | Train Acc 0.9328| Test Acc 0.8924\n",
      "Epoch 00020 | Loss 0.1949 | Train Acc 0.9343| Test Acc 0.8924\n",
      "Epoch 00021 | Loss 0.1869 | Train Acc 0.9366| Test Acc 0.8955\n",
      "Epoch 00022 | Loss 0.1795 | Train Acc 0.9396| Test Acc 0.8985\n",
      "Epoch 00023 | Loss 0.1725 | Train Acc 0.9433| Test Acc 0.9076\n",
      "Epoch 00024 | Loss 0.1658 | Train Acc 0.9463| Test Acc 0.9152\n",
      "Epoch 00025 | Loss 0.1596 | Train Acc 0.9507| Test Acc 0.9212\n",
      "Epoch 00026 | Loss 0.1537 | Train Acc 0.9515| Test Acc 0.9242\n",
      "Epoch 00027 | Loss 0.1481 | Train Acc 0.9530| Test Acc 0.9242\n",
      "Epoch 00028 | Loss 0.1429 | Train Acc 0.9552| Test Acc 0.9303\n",
      "Epoch 00029 | Loss 0.1380 | Train Acc 0.9575| Test Acc 0.9288\n",
      "Epoch 00030 | Loss 0.1335 | Train Acc 0.9597| Test Acc 0.9273\n",
      "Epoch 00031 | Loss 0.1292 | Train Acc 0.9619| Test Acc 0.9318\n",
      "Epoch 00032 | Loss 0.1252 | Train Acc 0.9627| Test Acc 0.9333\n",
      "Epoch 00033 | Loss 0.1215 | Train Acc 0.9657| Test Acc 0.9348\n",
      "Epoch 00034 | Loss 0.1179 | Train Acc 0.9679| Test Acc 0.9379\n",
      "Epoch 00035 | Loss 0.1145 | Train Acc 0.9687| Test Acc 0.9379\n",
      "Epoch 00036 | Loss 0.1113 | Train Acc 0.9694| Test Acc 0.9409\n",
      "Epoch 00037 | Loss 0.1081 | Train Acc 0.9701| Test Acc 0.9424\n",
      "Epoch 00038 | Loss 0.1052 | Train Acc 0.9709| Test Acc 0.9439\n",
      "Epoch 00039 | Loss 0.1023 | Train Acc 0.9724| Test Acc 0.9455\n",
      "Epoch 00040 | Loss 0.0996 | Train Acc 0.9731| Test Acc 0.9455\n",
      "Epoch 00041 | Loss 0.0969 | Train Acc 0.9739| Test Acc 0.9470\n",
      "Epoch 00042 | Loss 0.0943 | Train Acc 0.9746| Test Acc 0.9470\n",
      "Epoch 00043 | Loss 0.0918 | Train Acc 0.9746| Test Acc 0.9470\n",
      "Epoch 00044 | Loss 0.0893 | Train Acc 0.9746| Test Acc 0.9470\n",
      "Epoch 00045 | Loss 0.0869 | Train Acc 0.9754| Test Acc 0.9470\n",
      "Epoch 00046 | Loss 0.0846 | Train Acc 0.9761| Test Acc 0.9485\n",
      "Epoch 00047 | Loss 0.0823 | Train Acc 0.9776| Test Acc 0.9485\n",
      "Epoch 00048 | Loss 0.0801 | Train Acc 0.9806| Test Acc 0.9485\n",
      "Epoch 00049 | Loss 0.0780 | Train Acc 0.9806| Test Acc 0.9500\n",
      "Epoch 00050 | Loss 0.0759 | Train Acc 0.9806| Test Acc 0.9515\n",
      "Epoch 00051 | Loss 0.0739 | Train Acc 0.9813| Test Acc 0.9515\n",
      "Epoch 00052 | Loss 0.0720 | Train Acc 0.9828| Test Acc 0.9530\n",
      "Epoch 00053 | Loss 0.0701 | Train Acc 0.9836| Test Acc 0.9545\n",
      "Epoch 00054 | Loss 0.0683 | Train Acc 0.9843| Test Acc 0.9545\n",
      "Epoch 00055 | Loss 0.0665 | Train Acc 0.9851| Test Acc 0.9561\n",
      "Epoch 00056 | Loss 0.0648 | Train Acc 0.9851| Test Acc 0.9561\n",
      "Epoch 00057 | Loss 0.0632 | Train Acc 0.9843| Test Acc 0.9591\n",
      "Epoch 00058 | Loss 0.0615 | Train Acc 0.9858| Test Acc 0.9591\n",
      "Epoch 00059 | Loss 0.0600 | Train Acc 0.9866| Test Acc 0.9606\n",
      "Epoch 00060 | Loss 0.0584 | Train Acc 0.9866| Test Acc 0.9606\n",
      "Epoch 00061 | Loss 0.0569 | Train Acc 0.9873| Test Acc 0.9606\n",
      "Epoch 00062 | Loss 0.0555 | Train Acc 0.9881| Test Acc 0.9606\n",
      "Epoch 00063 | Loss 0.0541 | Train Acc 0.9888| Test Acc 0.9606\n",
      "Epoch 00064 | Loss 0.0527 | Train Acc 0.9903| Test Acc 0.9606\n",
      "Epoch 00065 | Loss 0.0514 | Train Acc 0.9903| Test Acc 0.9636\n",
      "Epoch 00066 | Loss 0.0501 | Train Acc 0.9903| Test Acc 0.9636\n",
      "Epoch 00067 | Loss 0.0488 | Train Acc 0.9903| Test Acc 0.9652\n",
      "Epoch 00068 | Loss 0.0476 | Train Acc 0.9903| Test Acc 0.9652\n",
      "Epoch 00069 | Loss 0.0465 | Train Acc 0.9903| Test Acc 0.9652\n",
      "Epoch 00070 | Loss 0.0453 | Train Acc 0.9903| Test Acc 0.9652\n",
      "Epoch 00071 | Loss 0.0442 | Train Acc 0.9903| Test Acc 0.9652\n",
      "Epoch 00072 | Loss 0.0431 | Train Acc 0.9903| Test Acc 0.9636\n",
      "Epoch 00073 | Loss 0.0421 | Train Acc 0.9910| Test Acc 0.9636\n",
      "Epoch 00074 | Loss 0.0410 | Train Acc 0.9918| Test Acc 0.9652\n",
      "Epoch 00075 | Loss 0.0400 | Train Acc 0.9918| Test Acc 0.9652\n",
      "Epoch 00076 | Loss 0.0390 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00077 | Loss 0.0381 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00078 | Loss 0.0372 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00079 | Loss 0.0363 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00080 | Loss 0.0354 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00081 | Loss 0.0345 | Train Acc 0.9940| Test Acc 0.9652\n",
      "Epoch 00082 | Loss 0.0337 | Train Acc 0.9948| Test Acc 0.9667\n",
      "Epoch 00083 | Loss 0.0329 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00084 | Loss 0.0321 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00085 | Loss 0.0313 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00086 | Loss 0.0306 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00087 | Loss 0.0298 | Train Acc 0.9963| Test Acc 0.9682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00088 | Loss 0.0291 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00089 | Loss 0.0284 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00090 | Loss 0.0277 | Train Acc 0.9963| Test Acc 0.9697\n",
      "Epoch 00091 | Loss 0.0270 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00092 | Loss 0.0263 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00093 | Loss 0.0257 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00094 | Loss 0.0250 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00095 | Loss 0.0244 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00096 | Loss 0.0238 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00097 | Loss 0.0232 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00098 | Loss 0.0227 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00099 | Loss 0.0221 | Train Acc 0.9970| Test Acc 0.9712\n",
      "Epoch 00100 | Loss 0.0216 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00101 | Loss 0.0211 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00102 | Loss 0.0206 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00103 | Loss 0.0201 | Train Acc 0.9978| Test Acc 0.9727\n",
      "Epoch 00104 | Loss 0.0196 | Train Acc 0.9978| Test Acc 0.9727\n",
      "Epoch 00105 | Loss 0.0192 | Train Acc 0.9978| Test Acc 0.9727\n",
      "Epoch 00106 | Loss 0.0187 | Train Acc 0.9978| Test Acc 0.9727\n",
      "Epoch 00107 | Loss 0.0183 | Train Acc 0.9985| Test Acc 0.9727\n",
      "Epoch 00108 | Loss 0.0179 | Train Acc 0.9985| Test Acc 0.9727\n",
      "Epoch 00109 | Loss 0.0174 | Train Acc 0.9985| Test Acc 0.9727\n",
      "Epoch 00110 | Loss 0.0170 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00111 | Loss 0.0166 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00112 | Loss 0.0163 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00113 | Loss 0.0159 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00114 | Loss 0.0155 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00115 | Loss 0.0152 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00116 | Loss 0.0149 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00117 | Loss 0.0146 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00118 | Loss 0.0142 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00119 | Loss 0.0139 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00120 | Loss 0.0136 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00121 | Loss 0.0133 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00122 | Loss 0.0131 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00123 | Loss 0.0128 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00124 | Loss 0.0125 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00125 | Loss 0.0123 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00126 | Loss 0.0120 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00127 | Loss 0.0118 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00128 | Loss 0.0115 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00129 | Loss 0.0113 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00130 | Loss 0.0111 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00131 | Loss 0.0108 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00132 | Loss 0.0106 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00133 | Loss 0.0104 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00134 | Loss 0.0102 | Train Acc 0.9993| Test Acc 0.9727\n",
      "Epoch 00135 | Loss 0.0100 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00136 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00137 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00138 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00139 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00140 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00141 | Loss 0.0089 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00142 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00143 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00144 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00145 | Loss 0.0083 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00146 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00147 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00148 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00149 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00150 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00151 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00152 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00153 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00154 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00155 | Loss 0.0070 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00156 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00157 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00158 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00159 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00160 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00161 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00162 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00163 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00164 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00165 | Loss 0.0059 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00166 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00167 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00168 | Loss 0.0056 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00169 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00170 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00171 | Loss 0.0054 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00172 | Loss 0.0053 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00173 | Loss 0.0052 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00174 | Loss 0.0051 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00175 | Loss 0.0051 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00176 | Loss 0.0050 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00177 | Loss 0.0049 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00178 | Loss 0.0048 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00179 | Loss 0.0048 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00180 | Loss 0.0047 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00181 | Loss 0.0046 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00182 | Loss 0.0046 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00183 | Loss 0.0045 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00184 | Loss 0.0044 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00185 | Loss 0.0044 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00186 | Loss 0.0043 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00187 | Loss 0.0043 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00188 | Loss 0.0042 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00189 | Loss 0.0041 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00190 | Loss 0.0041 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00191 | Loss 0.0040 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00192 | Loss 0.0040 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00193 | Loss 0.0039 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00194 | Loss 0.0039 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00195 | Loss 0.0038 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00196 | Loss 0.0038 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00197 | Loss 0.0037 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00198 | Loss 0.0037 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00199 | Loss 0.0036 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00000 | Loss 0.9562 | Train Acc 0.5396| Test Acc 0.5530\n",
      "Epoch 00001 | Loss 0.8004 | Train Acc 0.5791| Test Acc 0.6015\n",
      "Epoch 00002 | Loss 0.6913 | Train Acc 0.6396| Test Acc 0.6485\n",
      "Epoch 00003 | Loss 0.6114 | Train Acc 0.6985| Test Acc 0.7030\n",
      "Epoch 00004 | Loss 0.5526 | Train Acc 0.7433| Test Acc 0.7561\n",
      "Epoch 00005 | Loss 0.5069 | Train Acc 0.7731| Test Acc 0.7818\n",
      "Epoch 00006 | Loss 0.4683 | Train Acc 0.7985| Test Acc 0.7985\n",
      "Epoch 00007 | Loss 0.4338 | Train Acc 0.8119| Test Acc 0.8182\n",
      "Epoch 00008 | Loss 0.4024 | Train Acc 0.8254| Test Acc 0.8348\n",
      "Epoch 00009 | Loss 0.3740 | Train Acc 0.8403| Test Acc 0.8424\n",
      "Epoch 00010 | Loss 0.3490 | Train Acc 0.8522| Test Acc 0.8530\n",
      "Epoch 00011 | Loss 0.3273 | Train Acc 0.8619| Test Acc 0.8682\n",
      "Epoch 00012 | Loss 0.3089 | Train Acc 0.8694| Test Acc 0.8727\n",
      "Epoch 00013 | Loss 0.2932 | Train Acc 0.8791| Test Acc 0.8773\n",
      "Epoch 00014 | Loss 0.2797 | Train Acc 0.8866| Test Acc 0.8864\n",
      "Epoch 00015 | Loss 0.2679 | Train Acc 0.8910| Test Acc 0.8909\n",
      "Epoch 00016 | Loss 0.2572 | Train Acc 0.8955| Test Acc 0.8924\n",
      "Epoch 00017 | Loss 0.2472 | Train Acc 0.9015| Test Acc 0.8955\n",
      "Epoch 00018 | Loss 0.2378 | Train Acc 0.9060| Test Acc 0.9045\n",
      "Epoch 00019 | Loss 0.2288 | Train Acc 0.9134| Test Acc 0.9076\n",
      "Epoch 00020 | Loss 0.2203 | Train Acc 0.9201| Test Acc 0.9152\n",
      "Epoch 00021 | Loss 0.2122 | Train Acc 0.9246| Test Acc 0.9197\n",
      "Epoch 00022 | Loss 0.2046 | Train Acc 0.9299| Test Acc 0.9212\n",
      "Epoch 00023 | Loss 0.1976 | Train Acc 0.9328| Test Acc 0.9288\n",
      "Epoch 00024 | Loss 0.1912 | Train Acc 0.9351| Test Acc 0.9288\n",
      "Epoch 00025 | Loss 0.1852 | Train Acc 0.9381| Test Acc 0.9333\n",
      "Epoch 00026 | Loss 0.1797 | Train Acc 0.9381| Test Acc 0.9348\n",
      "Epoch 00027 | Loss 0.1746 | Train Acc 0.9410| Test Acc 0.9394\n",
      "Epoch 00028 | Loss 0.1697 | Train Acc 0.9410| Test Acc 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00029 | Loss 0.1650 | Train Acc 0.9440| Test Acc 0.9394\n",
      "Epoch 00030 | Loss 0.1606 | Train Acc 0.9455| Test Acc 0.9409\n",
      "Epoch 00031 | Loss 0.1562 | Train Acc 0.9485| Test Acc 0.9409\n",
      "Epoch 00032 | Loss 0.1521 | Train Acc 0.9530| Test Acc 0.9409\n",
      "Epoch 00033 | Loss 0.1481 | Train Acc 0.9545| Test Acc 0.9439\n",
      "Epoch 00034 | Loss 0.1443 | Train Acc 0.9560| Test Acc 0.9439\n",
      "Epoch 00035 | Loss 0.1406 | Train Acc 0.9560| Test Acc 0.9439\n",
      "Epoch 00036 | Loss 0.1372 | Train Acc 0.9575| Test Acc 0.9439\n",
      "Epoch 00037 | Loss 0.1339 | Train Acc 0.9582| Test Acc 0.9455\n",
      "Epoch 00038 | Loss 0.1308 | Train Acc 0.9604| Test Acc 0.9409\n",
      "Epoch 00039 | Loss 0.1278 | Train Acc 0.9604| Test Acc 0.9409\n",
      "Epoch 00040 | Loss 0.1249 | Train Acc 0.9612| Test Acc 0.9424\n",
      "Epoch 00041 | Loss 0.1221 | Train Acc 0.9619| Test Acc 0.9424\n",
      "Epoch 00042 | Loss 0.1194 | Train Acc 0.9634| Test Acc 0.9439\n",
      "Epoch 00043 | Loss 0.1168 | Train Acc 0.9649| Test Acc 0.9439\n",
      "Epoch 00044 | Loss 0.1142 | Train Acc 0.9657| Test Acc 0.9439\n",
      "Epoch 00045 | Loss 0.1116 | Train Acc 0.9657| Test Acc 0.9455\n",
      "Epoch 00046 | Loss 0.1092 | Train Acc 0.9672| Test Acc 0.9455\n",
      "Epoch 00047 | Loss 0.1068 | Train Acc 0.9672| Test Acc 0.9470\n",
      "Epoch 00048 | Loss 0.1045 | Train Acc 0.9672| Test Acc 0.9470\n",
      "Epoch 00049 | Loss 0.1023 | Train Acc 0.9687| Test Acc 0.9470\n",
      "Epoch 00050 | Loss 0.1002 | Train Acc 0.9694| Test Acc 0.9470\n",
      "Epoch 00051 | Loss 0.0981 | Train Acc 0.9701| Test Acc 0.9485\n",
      "Epoch 00052 | Loss 0.0960 | Train Acc 0.9709| Test Acc 0.9485\n",
      "Epoch 00053 | Loss 0.0940 | Train Acc 0.9709| Test Acc 0.9485\n",
      "Epoch 00054 | Loss 0.0920 | Train Acc 0.9716| Test Acc 0.9485\n",
      "Epoch 00055 | Loss 0.0901 | Train Acc 0.9724| Test Acc 0.9515\n",
      "Epoch 00056 | Loss 0.0882 | Train Acc 0.9739| Test Acc 0.9515\n",
      "Epoch 00057 | Loss 0.0864 | Train Acc 0.9761| Test Acc 0.9515\n",
      "Epoch 00058 | Loss 0.0846 | Train Acc 0.9761| Test Acc 0.9530\n",
      "Epoch 00059 | Loss 0.0829 | Train Acc 0.9769| Test Acc 0.9515\n",
      "Epoch 00060 | Loss 0.0812 | Train Acc 0.9784| Test Acc 0.9515\n",
      "Epoch 00061 | Loss 0.0795 | Train Acc 0.9791| Test Acc 0.9515\n",
      "Epoch 00062 | Loss 0.0779 | Train Acc 0.9791| Test Acc 0.9530\n",
      "Epoch 00063 | Loss 0.0763 | Train Acc 0.9791| Test Acc 0.9561\n",
      "Epoch 00064 | Loss 0.0747 | Train Acc 0.9813| Test Acc 0.9561\n",
      "Epoch 00065 | Loss 0.0732 | Train Acc 0.9813| Test Acc 0.9561\n",
      "Epoch 00066 | Loss 0.0716 | Train Acc 0.9821| Test Acc 0.9576\n",
      "Epoch 00067 | Loss 0.0701 | Train Acc 0.9828| Test Acc 0.9576\n",
      "Epoch 00068 | Loss 0.0687 | Train Acc 0.9836| Test Acc 0.9576\n",
      "Epoch 00069 | Loss 0.0673 | Train Acc 0.9836| Test Acc 0.9576\n",
      "Epoch 00070 | Loss 0.0659 | Train Acc 0.9836| Test Acc 0.9576\n",
      "Epoch 00071 | Loss 0.0645 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00072 | Loss 0.0632 | Train Acc 0.9858| Test Acc 0.9576\n",
      "Epoch 00073 | Loss 0.0619 | Train Acc 0.9873| Test Acc 0.9576\n",
      "Epoch 00074 | Loss 0.0606 | Train Acc 0.9873| Test Acc 0.9576\n",
      "Epoch 00075 | Loss 0.0594 | Train Acc 0.9873| Test Acc 0.9561\n",
      "Epoch 00076 | Loss 0.0582 | Train Acc 0.9873| Test Acc 0.9561\n",
      "Epoch 00077 | Loss 0.0570 | Train Acc 0.9881| Test Acc 0.9561\n",
      "Epoch 00078 | Loss 0.0558 | Train Acc 0.9881| Test Acc 0.9561\n",
      "Epoch 00079 | Loss 0.0546 | Train Acc 0.9881| Test Acc 0.9561\n",
      "Epoch 00080 | Loss 0.0535 | Train Acc 0.9888| Test Acc 0.9561\n",
      "Epoch 00081 | Loss 0.0524 | Train Acc 0.9888| Test Acc 0.9561\n",
      "Epoch 00082 | Loss 0.0513 | Train Acc 0.9888| Test Acc 0.9561\n",
      "Epoch 00083 | Loss 0.0503 | Train Acc 0.9888| Test Acc 0.9561\n",
      "Epoch 00084 | Loss 0.0492 | Train Acc 0.9888| Test Acc 0.9561\n",
      "Epoch 00085 | Loss 0.0482 | Train Acc 0.9888| Test Acc 0.9576\n",
      "Epoch 00086 | Loss 0.0473 | Train Acc 0.9888| Test Acc 0.9621\n",
      "Epoch 00087 | Loss 0.0463 | Train Acc 0.9888| Test Acc 0.9621\n",
      "Epoch 00088 | Loss 0.0453 | Train Acc 0.9888| Test Acc 0.9621\n",
      "Epoch 00089 | Loss 0.0444 | Train Acc 0.9896| Test Acc 0.9621\n",
      "Epoch 00090 | Loss 0.0435 | Train Acc 0.9903| Test Acc 0.9621\n",
      "Epoch 00091 | Loss 0.0426 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00092 | Loss 0.0418 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00093 | Loss 0.0409 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00094 | Loss 0.0401 | Train Acc 0.9910| Test Acc 0.9621\n",
      "Epoch 00095 | Loss 0.0393 | Train Acc 0.9910| Test Acc 0.9636\n",
      "Epoch 00096 | Loss 0.0385 | Train Acc 0.9918| Test Acc 0.9636\n",
      "Epoch 00097 | Loss 0.0377 | Train Acc 0.9925| Test Acc 0.9636\n",
      "Epoch 00098 | Loss 0.0369 | Train Acc 0.9925| Test Acc 0.9636\n",
      "Epoch 00099 | Loss 0.0361 | Train Acc 0.9940| Test Acc 0.9636\n",
      "Epoch 00100 | Loss 0.0354 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00101 | Loss 0.0346 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00102 | Loss 0.0339 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00103 | Loss 0.0331 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00104 | Loss 0.0325 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00105 | Loss 0.0318 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00106 | Loss 0.0311 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00107 | Loss 0.0305 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00108 | Loss 0.0298 | Train Acc 0.9963| Test Acc 0.9667\n",
      "Epoch 00109 | Loss 0.0292 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00110 | Loss 0.0286 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00111 | Loss 0.0279 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00112 | Loss 0.0274 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00113 | Loss 0.0268 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00114 | Loss 0.0262 | Train Acc 0.9970| Test Acc 0.9697\n",
      "Epoch 00115 | Loss 0.0256 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00116 | Loss 0.0251 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00117 | Loss 0.0246 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00118 | Loss 0.0241 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00119 | Loss 0.0236 | Train Acc 0.9978| Test Acc 0.9712\n",
      "Epoch 00120 | Loss 0.0231 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00121 | Loss 0.0226 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00122 | Loss 0.0221 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00123 | Loss 0.0217 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00124 | Loss 0.0212 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00125 | Loss 0.0208 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00126 | Loss 0.0204 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00127 | Loss 0.0199 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00128 | Loss 0.0195 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00129 | Loss 0.0192 | Train Acc 0.9985| Test Acc 0.9712\n",
      "Epoch 00130 | Loss 0.0188 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00131 | Loss 0.0184 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00132 | Loss 0.0180 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00133 | Loss 0.0177 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00134 | Loss 0.0174 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00135 | Loss 0.0170 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00136 | Loss 0.0167 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00137 | Loss 0.0164 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00138 | Loss 0.0161 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00139 | Loss 0.0158 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00140 | Loss 0.0155 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00141 | Loss 0.0152 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00142 | Loss 0.0149 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00143 | Loss 0.0146 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00144 | Loss 0.0143 | Train Acc 0.9993| Test Acc 0.9712\n",
      "Epoch 00145 | Loss 0.0141 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00146 | Loss 0.0138 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00147 | Loss 0.0136 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00148 | Loss 0.0133 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00149 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00150 | Loss 0.0128 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00151 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00152 | Loss 0.0124 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00153 | Loss 0.0122 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00154 | Loss 0.0120 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00155 | Loss 0.0118 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00156 | Loss 0.0116 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00157 | Loss 0.0114 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00158 | Loss 0.0112 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00159 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00160 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00161 | Loss 0.0106 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00162 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00163 | Loss 0.0103 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00164 | Loss 0.0101 | Train Acc 1.0000| Test Acc 0.9712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00165 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00166 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00167 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00168 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00169 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00170 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00171 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00172 | Loss 0.0089 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00173 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00174 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00175 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00176 | Loss 0.0083 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00177 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00178 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00179 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00180 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00181 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00182 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00183 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00184 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00185 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00186 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00187 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00188 | Loss 0.0070 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00189 | Loss 0.0069 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00190 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00191 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00192 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00193 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00194 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00195 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00196 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00197 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00198 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00199 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00000 | Loss 1.0560 | Train Acc 0.5060| Test Acc 0.5152\n",
      "Epoch 00001 | Loss 0.8969 | Train Acc 0.5440| Test Acc 0.5470\n",
      "Epoch 00002 | Loss 0.7862 | Train Acc 0.5843| Test Acc 0.5803\n",
      "Epoch 00003 | Loss 0.6960 | Train Acc 0.6231| Test Acc 0.6152\n",
      "Epoch 00004 | Loss 0.6233 | Train Acc 0.6731| Test Acc 0.6500\n",
      "Epoch 00005 | Loss 0.5642 | Train Acc 0.7075| Test Acc 0.6727\n",
      "Epoch 00006 | Loss 0.5148 | Train Acc 0.7321| Test Acc 0.6970\n",
      "Epoch 00007 | Loss 0.4729 | Train Acc 0.7530| Test Acc 0.7303\n",
      "Epoch 00008 | Loss 0.4371 | Train Acc 0.7813| Test Acc 0.7561\n",
      "Epoch 00009 | Loss 0.4063 | Train Acc 0.8082| Test Acc 0.7682\n",
      "Epoch 00010 | Loss 0.3800 | Train Acc 0.8306| Test Acc 0.7864\n",
      "Epoch 00011 | Loss 0.3574 | Train Acc 0.8433| Test Acc 0.8227\n",
      "Epoch 00012 | Loss 0.3380 | Train Acc 0.8597| Test Acc 0.8348\n",
      "Epoch 00013 | Loss 0.3211 | Train Acc 0.8761| Test Acc 0.8348\n",
      "Epoch 00014 | Loss 0.3062 | Train Acc 0.8821| Test Acc 0.8409\n",
      "Epoch 00015 | Loss 0.2929 | Train Acc 0.8903| Test Acc 0.8470\n",
      "Epoch 00016 | Loss 0.2809 | Train Acc 0.8963| Test Acc 0.8545\n",
      "Epoch 00017 | Loss 0.2697 | Train Acc 0.8970| Test Acc 0.8621\n",
      "Epoch 00018 | Loss 0.2595 | Train Acc 0.9030| Test Acc 0.8682\n",
      "Epoch 00019 | Loss 0.2499 | Train Acc 0.9090| Test Acc 0.8727\n",
      "Epoch 00020 | Loss 0.2411 | Train Acc 0.9142| Test Acc 0.8818\n",
      "Epoch 00021 | Loss 0.2328 | Train Acc 0.9164| Test Acc 0.8879\n",
      "Epoch 00022 | Loss 0.2250 | Train Acc 0.9172| Test Acc 0.8970\n",
      "Epoch 00023 | Loss 0.2177 | Train Acc 0.9179| Test Acc 0.8970\n",
      "Epoch 00024 | Loss 0.2107 | Train Acc 0.9194| Test Acc 0.8985\n",
      "Epoch 00025 | Loss 0.2040 | Train Acc 0.9216| Test Acc 0.8985\n",
      "Epoch 00026 | Loss 0.1976 | Train Acc 0.9224| Test Acc 0.9030\n",
      "Epoch 00027 | Loss 0.1915 | Train Acc 0.9269| Test Acc 0.9045\n",
      "Epoch 00028 | Loss 0.1856 | Train Acc 0.9284| Test Acc 0.9076\n",
      "Epoch 00029 | Loss 0.1799 | Train Acc 0.9321| Test Acc 0.9076\n",
      "Epoch 00030 | Loss 0.1745 | Train Acc 0.9328| Test Acc 0.9076\n",
      "Epoch 00031 | Loss 0.1693 | Train Acc 0.9366| Test Acc 0.9076\n",
      "Epoch 00032 | Loss 0.1644 | Train Acc 0.9403| Test Acc 0.9076\n",
      "Epoch 00033 | Loss 0.1598 | Train Acc 0.9440| Test Acc 0.9121\n",
      "Epoch 00034 | Loss 0.1553 | Train Acc 0.9440| Test Acc 0.9106\n",
      "Epoch 00035 | Loss 0.1511 | Train Acc 0.9463| Test Acc 0.9091\n",
      "Epoch 00036 | Loss 0.1470 | Train Acc 0.9485| Test Acc 0.9136\n",
      "Epoch 00037 | Loss 0.1431 | Train Acc 0.9485| Test Acc 0.9182\n",
      "Epoch 00038 | Loss 0.1394 | Train Acc 0.9507| Test Acc 0.9182\n",
      "Epoch 00039 | Loss 0.1357 | Train Acc 0.9522| Test Acc 0.9212\n",
      "Epoch 00040 | Loss 0.1322 | Train Acc 0.9522| Test Acc 0.9212\n",
      "Epoch 00041 | Loss 0.1288 | Train Acc 0.9530| Test Acc 0.9212\n",
      "Epoch 00042 | Loss 0.1255 | Train Acc 0.9552| Test Acc 0.9212\n",
      "Epoch 00043 | Loss 0.1224 | Train Acc 0.9567| Test Acc 0.9227\n",
      "Epoch 00044 | Loss 0.1193 | Train Acc 0.9575| Test Acc 0.9273\n",
      "Epoch 00045 | Loss 0.1164 | Train Acc 0.9582| Test Acc 0.9273\n",
      "Epoch 00046 | Loss 0.1135 | Train Acc 0.9604| Test Acc 0.9288\n",
      "Epoch 00047 | Loss 0.1107 | Train Acc 0.9619| Test Acc 0.9318\n",
      "Epoch 00048 | Loss 0.1080 | Train Acc 0.9627| Test Acc 0.9318\n",
      "Epoch 00049 | Loss 0.1054 | Train Acc 0.9634| Test Acc 0.9318\n",
      "Epoch 00050 | Loss 0.1029 | Train Acc 0.9642| Test Acc 0.9318\n",
      "Epoch 00051 | Loss 0.1004 | Train Acc 0.9649| Test Acc 0.9348\n",
      "Epoch 00052 | Loss 0.0980 | Train Acc 0.9672| Test Acc 0.9364\n",
      "Epoch 00053 | Loss 0.0957 | Train Acc 0.9687| Test Acc 0.9364\n",
      "Epoch 00054 | Loss 0.0934 | Train Acc 0.9701| Test Acc 0.9364\n",
      "Epoch 00055 | Loss 0.0912 | Train Acc 0.9709| Test Acc 0.9394\n",
      "Epoch 00056 | Loss 0.0890 | Train Acc 0.9709| Test Acc 0.9409\n",
      "Epoch 00057 | Loss 0.0869 | Train Acc 0.9724| Test Acc 0.9409\n",
      "Epoch 00058 | Loss 0.0849 | Train Acc 0.9731| Test Acc 0.9409\n",
      "Epoch 00059 | Loss 0.0829 | Train Acc 0.9739| Test Acc 0.9424\n",
      "Epoch 00060 | Loss 0.0809 | Train Acc 0.9761| Test Acc 0.9424\n",
      "Epoch 00061 | Loss 0.0790 | Train Acc 0.9761| Test Acc 0.9424\n",
      "Epoch 00062 | Loss 0.0772 | Train Acc 0.9769| Test Acc 0.9424\n",
      "Epoch 00063 | Loss 0.0753 | Train Acc 0.9769| Test Acc 0.9424\n",
      "Epoch 00064 | Loss 0.0736 | Train Acc 0.9784| Test Acc 0.9424\n",
      "Epoch 00065 | Loss 0.0719 | Train Acc 0.9799| Test Acc 0.9439\n",
      "Epoch 00066 | Loss 0.0702 | Train Acc 0.9813| Test Acc 0.9455\n",
      "Epoch 00067 | Loss 0.0685 | Train Acc 0.9828| Test Acc 0.9485\n",
      "Epoch 00068 | Loss 0.0669 | Train Acc 0.9828| Test Acc 0.9500\n",
      "Epoch 00069 | Loss 0.0653 | Train Acc 0.9828| Test Acc 0.9530\n",
      "Epoch 00070 | Loss 0.0638 | Train Acc 0.9836| Test Acc 0.9530\n",
      "Epoch 00071 | Loss 0.0623 | Train Acc 0.9851| Test Acc 0.9530\n",
      "Epoch 00072 | Loss 0.0609 | Train Acc 0.9858| Test Acc 0.9530\n",
      "Epoch 00073 | Loss 0.0595 | Train Acc 0.9873| Test Acc 0.9530\n",
      "Epoch 00074 | Loss 0.0581 | Train Acc 0.9873| Test Acc 0.9530\n",
      "Epoch 00075 | Loss 0.0568 | Train Acc 0.9888| Test Acc 0.9530\n",
      "Epoch 00076 | Loss 0.0555 | Train Acc 0.9910| Test Acc 0.9530\n",
      "Epoch 00077 | Loss 0.0542 | Train Acc 0.9910| Test Acc 0.9545\n",
      "Epoch 00078 | Loss 0.0529 | Train Acc 0.9910| Test Acc 0.9561\n",
      "Epoch 00079 | Loss 0.0517 | Train Acc 0.9910| Test Acc 0.9561\n",
      "Epoch 00080 | Loss 0.0506 | Train Acc 0.9910| Test Acc 0.9561\n",
      "Epoch 00081 | Loss 0.0494 | Train Acc 0.9910| Test Acc 0.9576\n",
      "Epoch 00082 | Loss 0.0483 | Train Acc 0.9910| Test Acc 0.9576\n",
      "Epoch 00083 | Loss 0.0472 | Train Acc 0.9918| Test Acc 0.9576\n",
      "Epoch 00084 | Loss 0.0461 | Train Acc 0.9918| Test Acc 0.9576\n",
      "Epoch 00085 | Loss 0.0451 | Train Acc 0.9925| Test Acc 0.9576\n",
      "Epoch 00086 | Loss 0.0441 | Train Acc 0.9940| Test Acc 0.9606\n",
      "Epoch 00087 | Loss 0.0431 | Train Acc 0.9940| Test Acc 0.9606\n",
      "Epoch 00088 | Loss 0.0421 | Train Acc 0.9940| Test Acc 0.9621\n",
      "Epoch 00089 | Loss 0.0412 | Train Acc 0.9940| Test Acc 0.9621\n",
      "Epoch 00090 | Loss 0.0403 | Train Acc 0.9940| Test Acc 0.9621\n",
      "Epoch 00091 | Loss 0.0393 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00092 | Loss 0.0385 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00093 | Loss 0.0376 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00094 | Loss 0.0367 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00095 | Loss 0.0359 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00096 | Loss 0.0351 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00097 | Loss 0.0343 | Train Acc 0.9948| Test Acc 0.9621\n",
      "Epoch 00098 | Loss 0.0336 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00099 | Loss 0.0328 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00100 | Loss 0.0321 | Train Acc 0.9955| Test Acc 0.9621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00101 | Loss 0.0314 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00102 | Loss 0.0307 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00103 | Loss 0.0300 | Train Acc 0.9955| Test Acc 0.9621\n",
      "Epoch 00104 | Loss 0.0293 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00105 | Loss 0.0287 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00106 | Loss 0.0280 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00107 | Loss 0.0274 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00108 | Loss 0.0268 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00109 | Loss 0.0262 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00110 | Loss 0.0257 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00111 | Loss 0.0251 | Train Acc 0.9955| Test Acc 0.9652\n",
      "Epoch 00112 | Loss 0.0246 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00113 | Loss 0.0240 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00114 | Loss 0.0235 | Train Acc 0.9955| Test Acc 0.9667\n",
      "Epoch 00115 | Loss 0.0230 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00116 | Loss 0.0225 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00117 | Loss 0.0220 | Train Acc 0.9978| Test Acc 0.9682\n",
      "Epoch 00118 | Loss 0.0216 | Train Acc 0.9985| Test Acc 0.9682\n",
      "Epoch 00119 | Loss 0.0211 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00120 | Loss 0.0207 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00121 | Loss 0.0202 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00122 | Loss 0.0198 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00123 | Loss 0.0194 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00124 | Loss 0.0190 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00125 | Loss 0.0186 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00126 | Loss 0.0182 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00127 | Loss 0.0179 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00128 | Loss 0.0175 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00129 | Loss 0.0172 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00130 | Loss 0.0168 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00131 | Loss 0.0165 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00132 | Loss 0.0161 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00133 | Loss 0.0158 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00134 | Loss 0.0155 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00135 | Loss 0.0152 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00136 | Loss 0.0149 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00137 | Loss 0.0146 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00138 | Loss 0.0143 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00139 | Loss 0.0141 | Train Acc 0.9993| Test Acc 0.9682\n",
      "Epoch 00140 | Loss 0.0138 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00141 | Loss 0.0135 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00142 | Loss 0.0133 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00143 | Loss 0.0130 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00144 | Loss 0.0128 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00145 | Loss 0.0126 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00146 | Loss 0.0123 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00147 | Loss 0.0121 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00148 | Loss 0.0119 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00149 | Loss 0.0117 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00150 | Loss 0.0114 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00151 | Loss 0.0112 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00152 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00153 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00154 | Loss 0.0107 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00155 | Loss 0.0105 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00156 | Loss 0.0103 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00157 | Loss 0.0101 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00158 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00159 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00160 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00161 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00162 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00163 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00164 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00165 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00166 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00167 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00168 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00169 | Loss 0.0083 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00170 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00171 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00172 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00173 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00174 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00175 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00176 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00177 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00178 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00179 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00180 | Loss 0.0070 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00181 | Loss 0.0069 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00182 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00183 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00184 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00185 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00186 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00187 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00188 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00189 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00190 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00191 | Loss 0.0059 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00192 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00193 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00194 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00195 | Loss 0.0056 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00196 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00197 | Loss 0.0054 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00198 | Loss 0.0053 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00199 | Loss 0.0053 | Train Acc 1.0000| Test Acc 0.9682\n",
      "Epoch 00000 | Loss 1.0432 | Train Acc 0.4836| Test Acc 0.4576\n",
      "Epoch 00001 | Loss 0.8913 | Train Acc 0.5313| Test Acc 0.4955\n",
      "Epoch 00002 | Loss 0.7803 | Train Acc 0.5866| Test Acc 0.5439\n",
      "Epoch 00003 | Loss 0.6843 | Train Acc 0.6470| Test Acc 0.6045\n",
      "Epoch 00004 | Loss 0.6029 | Train Acc 0.7060| Test Acc 0.6545\n",
      "Epoch 00005 | Loss 0.5350 | Train Acc 0.7739| Test Acc 0.6970\n",
      "Epoch 00006 | Loss 0.4790 | Train Acc 0.8060| Test Acc 0.7455\n",
      "Epoch 00007 | Loss 0.4328 | Train Acc 0.8306| Test Acc 0.7848\n",
      "Epoch 00008 | Loss 0.3942 | Train Acc 0.8485| Test Acc 0.8045\n",
      "Epoch 00009 | Loss 0.3617 | Train Acc 0.8627| Test Acc 0.8258\n",
      "Epoch 00010 | Loss 0.3341 | Train Acc 0.8791| Test Acc 0.8409\n",
      "Epoch 00011 | Loss 0.3106 | Train Acc 0.8896| Test Acc 0.8545\n",
      "Epoch 00012 | Loss 0.2904 | Train Acc 0.8985| Test Acc 0.8606\n",
      "Epoch 00013 | Loss 0.2730 | Train Acc 0.9052| Test Acc 0.8682\n",
      "Epoch 00014 | Loss 0.2578 | Train Acc 0.9104| Test Acc 0.8803\n",
      "Epoch 00015 | Loss 0.2444 | Train Acc 0.9164| Test Acc 0.8833\n",
      "Epoch 00016 | Loss 0.2326 | Train Acc 0.9239| Test Acc 0.8848\n",
      "Epoch 00017 | Loss 0.2220 | Train Acc 0.9269| Test Acc 0.8909\n",
      "Epoch 00018 | Loss 0.2124 | Train Acc 0.9299| Test Acc 0.8985\n",
      "Epoch 00019 | Loss 0.2038 | Train Acc 0.9351| Test Acc 0.8985\n",
      "Epoch 00020 | Loss 0.1959 | Train Acc 0.9396| Test Acc 0.9045\n",
      "Epoch 00021 | Loss 0.1886 | Train Acc 0.9403| Test Acc 0.9061\n",
      "Epoch 00022 | Loss 0.1817 | Train Acc 0.9410| Test Acc 0.9152\n",
      "Epoch 00023 | Loss 0.1753 | Train Acc 0.9455| Test Acc 0.9197\n",
      "Epoch 00024 | Loss 0.1693 | Train Acc 0.9478| Test Acc 0.9212\n",
      "Epoch 00025 | Loss 0.1637 | Train Acc 0.9463| Test Acc 0.9242\n",
      "Epoch 00026 | Loss 0.1584 | Train Acc 0.9470| Test Acc 0.9273\n",
      "Epoch 00027 | Loss 0.1534 | Train Acc 0.9493| Test Acc 0.9288\n",
      "Epoch 00028 | Loss 0.1487 | Train Acc 0.9522| Test Acc 0.9303\n",
      "Epoch 00029 | Loss 0.1442 | Train Acc 0.9522| Test Acc 0.9303\n",
      "Epoch 00030 | Loss 0.1400 | Train Acc 0.9537| Test Acc 0.9288\n",
      "Epoch 00031 | Loss 0.1360 | Train Acc 0.9545| Test Acc 0.9288\n",
      "Epoch 00032 | Loss 0.1322 | Train Acc 0.9560| Test Acc 0.9288\n",
      "Epoch 00033 | Loss 0.1286 | Train Acc 0.9567| Test Acc 0.9303\n",
      "Epoch 00034 | Loss 0.1252 | Train Acc 0.9582| Test Acc 0.9348\n",
      "Epoch 00035 | Loss 0.1219 | Train Acc 0.9582| Test Acc 0.9348\n",
      "Epoch 00036 | Loss 0.1188 | Train Acc 0.9582| Test Acc 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00037 | Loss 0.1158 | Train Acc 0.9604| Test Acc 0.9333\n",
      "Epoch 00038 | Loss 0.1130 | Train Acc 0.9612| Test Acc 0.9348\n",
      "Epoch 00039 | Loss 0.1103 | Train Acc 0.9649| Test Acc 0.9348\n",
      "Epoch 00040 | Loss 0.1077 | Train Acc 0.9657| Test Acc 0.9348\n",
      "Epoch 00041 | Loss 0.1052 | Train Acc 0.9687| Test Acc 0.9333\n",
      "Epoch 00042 | Loss 0.1027 | Train Acc 0.9687| Test Acc 0.9348\n",
      "Epoch 00043 | Loss 0.1004 | Train Acc 0.9687| Test Acc 0.9348\n",
      "Epoch 00044 | Loss 0.0980 | Train Acc 0.9694| Test Acc 0.9348\n",
      "Epoch 00045 | Loss 0.0958 | Train Acc 0.9716| Test Acc 0.9348\n",
      "Epoch 00046 | Loss 0.0936 | Train Acc 0.9724| Test Acc 0.9333\n",
      "Epoch 00047 | Loss 0.0915 | Train Acc 0.9731| Test Acc 0.9333\n",
      "Epoch 00048 | Loss 0.0894 | Train Acc 0.9739| Test Acc 0.9348\n",
      "Epoch 00049 | Loss 0.0874 | Train Acc 0.9746| Test Acc 0.9364\n",
      "Epoch 00050 | Loss 0.0854 | Train Acc 0.9746| Test Acc 0.9379\n",
      "Epoch 00051 | Loss 0.0835 | Train Acc 0.9746| Test Acc 0.9379\n",
      "Epoch 00052 | Loss 0.0816 | Train Acc 0.9746| Test Acc 0.9394\n",
      "Epoch 00053 | Loss 0.0798 | Train Acc 0.9754| Test Acc 0.9409\n",
      "Epoch 00054 | Loss 0.0780 | Train Acc 0.9754| Test Acc 0.9409\n",
      "Epoch 00055 | Loss 0.0763 | Train Acc 0.9754| Test Acc 0.9424\n",
      "Epoch 00056 | Loss 0.0746 | Train Acc 0.9769| Test Acc 0.9424\n",
      "Epoch 00057 | Loss 0.0729 | Train Acc 0.9776| Test Acc 0.9424\n",
      "Epoch 00058 | Loss 0.0713 | Train Acc 0.9784| Test Acc 0.9409\n",
      "Epoch 00059 | Loss 0.0698 | Train Acc 0.9791| Test Acc 0.9439\n",
      "Epoch 00060 | Loss 0.0682 | Train Acc 0.9799| Test Acc 0.9439\n",
      "Epoch 00061 | Loss 0.0667 | Train Acc 0.9799| Test Acc 0.9439\n",
      "Epoch 00062 | Loss 0.0653 | Train Acc 0.9806| Test Acc 0.9470\n",
      "Epoch 00063 | Loss 0.0639 | Train Acc 0.9806| Test Acc 0.9470\n",
      "Epoch 00064 | Loss 0.0625 | Train Acc 0.9806| Test Acc 0.9470\n",
      "Epoch 00065 | Loss 0.0611 | Train Acc 0.9836| Test Acc 0.9470\n",
      "Epoch 00066 | Loss 0.0598 | Train Acc 0.9836| Test Acc 0.9470\n",
      "Epoch 00067 | Loss 0.0584 | Train Acc 0.9843| Test Acc 0.9470\n",
      "Epoch 00068 | Loss 0.0572 | Train Acc 0.9843| Test Acc 0.9485\n",
      "Epoch 00069 | Loss 0.0559 | Train Acc 0.9858| Test Acc 0.9485\n",
      "Epoch 00070 | Loss 0.0547 | Train Acc 0.9858| Test Acc 0.9470\n",
      "Epoch 00071 | Loss 0.0535 | Train Acc 0.9858| Test Acc 0.9485\n",
      "Epoch 00072 | Loss 0.0523 | Train Acc 0.9858| Test Acc 0.9485\n",
      "Epoch 00073 | Loss 0.0511 | Train Acc 0.9858| Test Acc 0.9485\n",
      "Epoch 00074 | Loss 0.0500 | Train Acc 0.9858| Test Acc 0.9500\n",
      "Epoch 00075 | Loss 0.0489 | Train Acc 0.9866| Test Acc 0.9515\n",
      "Epoch 00076 | Loss 0.0478 | Train Acc 0.9866| Test Acc 0.9515\n",
      "Epoch 00077 | Loss 0.0467 | Train Acc 0.9873| Test Acc 0.9515\n",
      "Epoch 00078 | Loss 0.0457 | Train Acc 0.9881| Test Acc 0.9515\n",
      "Epoch 00079 | Loss 0.0446 | Train Acc 0.9881| Test Acc 0.9515\n",
      "Epoch 00080 | Loss 0.0436 | Train Acc 0.9888| Test Acc 0.9530\n",
      "Epoch 00081 | Loss 0.0427 | Train Acc 0.9888| Test Acc 0.9530\n",
      "Epoch 00082 | Loss 0.0417 | Train Acc 0.9896| Test Acc 0.9545\n",
      "Epoch 00083 | Loss 0.0408 | Train Acc 0.9896| Test Acc 0.9545\n",
      "Epoch 00084 | Loss 0.0399 | Train Acc 0.9896| Test Acc 0.9530\n",
      "Epoch 00085 | Loss 0.0390 | Train Acc 0.9896| Test Acc 0.9545\n",
      "Epoch 00086 | Loss 0.0381 | Train Acc 0.9896| Test Acc 0.9545\n",
      "Epoch 00087 | Loss 0.0373 | Train Acc 0.9896| Test Acc 0.9561\n",
      "Epoch 00088 | Loss 0.0365 | Train Acc 0.9910| Test Acc 0.9561\n",
      "Epoch 00089 | Loss 0.0357 | Train Acc 0.9918| Test Acc 0.9561\n",
      "Epoch 00090 | Loss 0.0349 | Train Acc 0.9925| Test Acc 0.9561\n",
      "Epoch 00091 | Loss 0.0341 | Train Acc 0.9925| Test Acc 0.9561\n",
      "Epoch 00092 | Loss 0.0333 | Train Acc 0.9940| Test Acc 0.9561\n",
      "Epoch 00093 | Loss 0.0326 | Train Acc 0.9940| Test Acc 0.9561\n",
      "Epoch 00094 | Loss 0.0319 | Train Acc 0.9948| Test Acc 0.9576\n",
      "Epoch 00095 | Loss 0.0311 | Train Acc 0.9948| Test Acc 0.9576\n",
      "Epoch 00096 | Loss 0.0304 | Train Acc 0.9948| Test Acc 0.9576\n",
      "Epoch 00097 | Loss 0.0298 | Train Acc 0.9948| Test Acc 0.9576\n",
      "Epoch 00098 | Loss 0.0291 | Train Acc 0.9948| Test Acc 0.9591\n",
      "Epoch 00099 | Loss 0.0284 | Train Acc 0.9955| Test Acc 0.9591\n",
      "Epoch 00100 | Loss 0.0278 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00101 | Loss 0.0272 | Train Acc 0.9963| Test Acc 0.9576\n",
      "Epoch 00102 | Loss 0.0266 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00103 | Loss 0.0260 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00104 | Loss 0.0254 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00105 | Loss 0.0248 | Train Acc 0.9963| Test Acc 0.9591\n",
      "Epoch 00106 | Loss 0.0243 | Train Acc 0.9963| Test Acc 0.9606\n",
      "Epoch 00107 | Loss 0.0238 | Train Acc 0.9963| Test Acc 0.9606\n",
      "Epoch 00108 | Loss 0.0232 | Train Acc 0.9963| Test Acc 0.9621\n",
      "Epoch 00109 | Loss 0.0227 | Train Acc 0.9963| Test Acc 0.9621\n",
      "Epoch 00110 | Loss 0.0222 | Train Acc 0.9963| Test Acc 0.9606\n",
      "Epoch 00111 | Loss 0.0217 | Train Acc 0.9970| Test Acc 0.9606\n",
      "Epoch 00112 | Loss 0.0213 | Train Acc 0.9985| Test Acc 0.9606\n",
      "Epoch 00113 | Loss 0.0208 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00114 | Loss 0.0203 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00115 | Loss 0.0199 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00116 | Loss 0.0195 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00117 | Loss 0.0191 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00118 | Loss 0.0186 | Train Acc 0.9993| Test Acc 0.9606\n",
      "Epoch 00119 | Loss 0.0183 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00120 | Loss 0.0179 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00121 | Loss 0.0175 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00122 | Loss 0.0171 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00123 | Loss 0.0168 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00124 | Loss 0.0164 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00125 | Loss 0.0161 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00126 | Loss 0.0157 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00127 | Loss 0.0154 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00128 | Loss 0.0151 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00129 | Loss 0.0148 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00130 | Loss 0.0145 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00131 | Loss 0.0142 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00132 | Loss 0.0139 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00133 | Loss 0.0137 | Train Acc 1.0000| Test Acc 0.9606\n",
      "Epoch 00134 | Loss 0.0134 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00135 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00136 | Loss 0.0129 | Train Acc 1.0000| Test Acc 0.9621\n",
      "Epoch 00137 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00138 | Loss 0.0124 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00139 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00140 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00141 | Loss 0.0117 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00142 | Loss 0.0115 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00143 | Loss 0.0113 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00144 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00145 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00146 | Loss 0.0106 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00147 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00148 | Loss 0.0103 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00149 | Loss 0.0101 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00150 | Loss 0.0099 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00151 | Loss 0.0097 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00152 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00153 | Loss 0.0094 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00154 | Loss 0.0092 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00155 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00156 | Loss 0.0089 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00157 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00158 | Loss 0.0086 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00159 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00160 | Loss 0.0083 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00161 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00162 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00163 | Loss 0.0079 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00164 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00165 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00166 | Loss 0.0075 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00167 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00168 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00169 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00170 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00171 | Loss 0.0069 | Train Acc 1.0000| Test Acc 0.9636\n",
      "Epoch 00172 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00173 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00174 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00175 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00176 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00177 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00178 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00179 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00180 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00181 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00182 | Loss 0.0059 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00183 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00184 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00185 | Loss 0.0056 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00186 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00187 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00188 | Loss 0.0054 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00189 | Loss 0.0053 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00190 | Loss 0.0052 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00191 | Loss 0.0051 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00192 | Loss 0.0051 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00193 | Loss 0.0050 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00194 | Loss 0.0049 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00195 | Loss 0.0049 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00196 | Loss 0.0048 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00197 | Loss 0.0047 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00198 | Loss 0.0047 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00199 | Loss 0.0046 | Train Acc 1.0000| Test Acc 0.9652\n",
      "Epoch 00000 | Loss 1.1662 | Train Acc 0.4843| Test Acc 0.5106\n",
      "Epoch 00001 | Loss 0.9756 | Train Acc 0.5373| Test Acc 0.5455\n",
      "Epoch 00002 | Loss 0.8561 | Train Acc 0.5843| Test Acc 0.5894\n",
      "Epoch 00003 | Loss 0.7593 | Train Acc 0.6351| Test Acc 0.6152\n",
      "Epoch 00004 | Loss 0.6804 | Train Acc 0.6687| Test Acc 0.6576\n",
      "Epoch 00005 | Loss 0.6147 | Train Acc 0.7060| Test Acc 0.6909\n",
      "Epoch 00006 | Loss 0.5585 | Train Acc 0.7366| Test Acc 0.7212\n",
      "Epoch 00007 | Loss 0.5096 | Train Acc 0.7664| Test Acc 0.7545\n",
      "Epoch 00008 | Loss 0.4665 | Train Acc 0.7910| Test Acc 0.7697\n",
      "Epoch 00009 | Loss 0.4286 | Train Acc 0.8112| Test Acc 0.7879\n",
      "Epoch 00010 | Loss 0.3958 | Train Acc 0.8328| Test Acc 0.8106\n",
      "Epoch 00011 | Loss 0.3676 | Train Acc 0.8493| Test Acc 0.8212\n",
      "Epoch 00012 | Loss 0.3438 | Train Acc 0.8634| Test Acc 0.8379\n",
      "Epoch 00013 | Loss 0.3238 | Train Acc 0.8716| Test Acc 0.8530\n",
      "Epoch 00014 | Loss 0.3070 | Train Acc 0.8799| Test Acc 0.8621\n",
      "Epoch 00015 | Loss 0.2925 | Train Acc 0.8873| Test Acc 0.8652\n",
      "Epoch 00016 | Loss 0.2798 | Train Acc 0.8933| Test Acc 0.8727\n",
      "Epoch 00017 | Loss 0.2685 | Train Acc 0.9015| Test Acc 0.8803\n",
      "Epoch 00018 | Loss 0.2581 | Train Acc 0.9045| Test Acc 0.8848\n",
      "Epoch 00019 | Loss 0.2484 | Train Acc 0.9075| Test Acc 0.8909\n",
      "Epoch 00020 | Loss 0.2395 | Train Acc 0.9112| Test Acc 0.8939\n",
      "Epoch 00021 | Loss 0.2311 | Train Acc 0.9149| Test Acc 0.9000\n",
      "Epoch 00022 | Loss 0.2233 | Train Acc 0.9149| Test Acc 0.9045\n",
      "Epoch 00023 | Loss 0.2161 | Train Acc 0.9164| Test Acc 0.9045\n",
      "Epoch 00024 | Loss 0.2094 | Train Acc 0.9254| Test Acc 0.9152\n",
      "Epoch 00025 | Loss 0.2032 | Train Acc 0.9284| Test Acc 0.9152\n",
      "Epoch 00026 | Loss 0.1974 | Train Acc 0.9313| Test Acc 0.9197\n",
      "Epoch 00027 | Loss 0.1919 | Train Acc 0.9328| Test Acc 0.9212\n",
      "Epoch 00028 | Loss 0.1866 | Train Acc 0.9351| Test Acc 0.9242\n",
      "Epoch 00029 | Loss 0.1816 | Train Acc 0.9388| Test Acc 0.9242\n",
      "Epoch 00030 | Loss 0.1767 | Train Acc 0.9403| Test Acc 0.9227\n",
      "Epoch 00031 | Loss 0.1720 | Train Acc 0.9425| Test Acc 0.9273\n",
      "Epoch 00032 | Loss 0.1674 | Train Acc 0.9440| Test Acc 0.9303\n",
      "Epoch 00033 | Loss 0.1631 | Train Acc 0.9448| Test Acc 0.9318\n",
      "Epoch 00034 | Loss 0.1590 | Train Acc 0.9470| Test Acc 0.9318\n",
      "Epoch 00035 | Loss 0.1550 | Train Acc 0.9500| Test Acc 0.9333\n",
      "Epoch 00036 | Loss 0.1512 | Train Acc 0.9515| Test Acc 0.9333\n",
      "Epoch 00037 | Loss 0.1476 | Train Acc 0.9515| Test Acc 0.9348\n",
      "Epoch 00038 | Loss 0.1441 | Train Acc 0.9515| Test Acc 0.9364\n",
      "Epoch 00039 | Loss 0.1408 | Train Acc 0.9537| Test Acc 0.9364\n",
      "Epoch 00040 | Loss 0.1376 | Train Acc 0.9552| Test Acc 0.9364\n",
      "Epoch 00041 | Loss 0.1345 | Train Acc 0.9567| Test Acc 0.9364\n",
      "Epoch 00042 | Loss 0.1315 | Train Acc 0.9590| Test Acc 0.9364\n",
      "Epoch 00043 | Loss 0.1285 | Train Acc 0.9597| Test Acc 0.9364\n",
      "Epoch 00044 | Loss 0.1257 | Train Acc 0.9619| Test Acc 0.9364\n",
      "Epoch 00045 | Loss 0.1229 | Train Acc 0.9619| Test Acc 0.9364\n",
      "Epoch 00046 | Loss 0.1202 | Train Acc 0.9634| Test Acc 0.9364\n",
      "Epoch 00047 | Loss 0.1177 | Train Acc 0.9642| Test Acc 0.9394\n",
      "Epoch 00048 | Loss 0.1152 | Train Acc 0.9649| Test Acc 0.9394\n",
      "Epoch 00049 | Loss 0.1127 | Train Acc 0.9657| Test Acc 0.9394\n",
      "Epoch 00050 | Loss 0.1103 | Train Acc 0.9664| Test Acc 0.9379\n",
      "Epoch 00051 | Loss 0.1080 | Train Acc 0.9664| Test Acc 0.9379\n",
      "Epoch 00052 | Loss 0.1057 | Train Acc 0.9679| Test Acc 0.9394\n",
      "Epoch 00053 | Loss 0.1035 | Train Acc 0.9694| Test Acc 0.9394\n",
      "Epoch 00054 | Loss 0.1013 | Train Acc 0.9716| Test Acc 0.9409\n",
      "Epoch 00055 | Loss 0.0992 | Train Acc 0.9731| Test Acc 0.9409\n",
      "Epoch 00056 | Loss 0.0971 | Train Acc 0.9731| Test Acc 0.9409\n",
      "Epoch 00057 | Loss 0.0951 | Train Acc 0.9731| Test Acc 0.9409\n",
      "Epoch 00058 | Loss 0.0931 | Train Acc 0.9746| Test Acc 0.9409\n",
      "Epoch 00059 | Loss 0.0911 | Train Acc 0.9754| Test Acc 0.9439\n",
      "Epoch 00060 | Loss 0.0892 | Train Acc 0.9761| Test Acc 0.9439\n",
      "Epoch 00061 | Loss 0.0873 | Train Acc 0.9761| Test Acc 0.9470\n",
      "Epoch 00062 | Loss 0.0855 | Train Acc 0.9761| Test Acc 0.9500\n",
      "Epoch 00063 | Loss 0.0837 | Train Acc 0.9776| Test Acc 0.9500\n",
      "Epoch 00064 | Loss 0.0819 | Train Acc 0.9784| Test Acc 0.9500\n",
      "Epoch 00065 | Loss 0.0802 | Train Acc 0.9784| Test Acc 0.9515\n",
      "Epoch 00066 | Loss 0.0785 | Train Acc 0.9791| Test Acc 0.9515\n",
      "Epoch 00067 | Loss 0.0768 | Train Acc 0.9791| Test Acc 0.9515\n",
      "Epoch 00068 | Loss 0.0752 | Train Acc 0.9791| Test Acc 0.9515\n",
      "Epoch 00069 | Loss 0.0736 | Train Acc 0.9791| Test Acc 0.9530\n",
      "Epoch 00070 | Loss 0.0720 | Train Acc 0.9799| Test Acc 0.9545\n",
      "Epoch 00071 | Loss 0.0705 | Train Acc 0.9799| Test Acc 0.9545\n",
      "Epoch 00072 | Loss 0.0690 | Train Acc 0.9799| Test Acc 0.9545\n",
      "Epoch 00073 | Loss 0.0675 | Train Acc 0.9813| Test Acc 0.9545\n",
      "Epoch 00074 | Loss 0.0660 | Train Acc 0.9821| Test Acc 0.9545\n",
      "Epoch 00075 | Loss 0.0646 | Train Acc 0.9821| Test Acc 0.9576\n",
      "Epoch 00076 | Loss 0.0632 | Train Acc 0.9821| Test Acc 0.9591\n",
      "Epoch 00077 | Loss 0.0619 | Train Acc 0.9828| Test Acc 0.9621\n",
      "Epoch 00078 | Loss 0.0605 | Train Acc 0.9836| Test Acc 0.9621\n",
      "Epoch 00079 | Loss 0.0592 | Train Acc 0.9843| Test Acc 0.9621\n",
      "Epoch 00080 | Loss 0.0579 | Train Acc 0.9851| Test Acc 0.9621\n",
      "Epoch 00081 | Loss 0.0567 | Train Acc 0.9851| Test Acc 0.9636\n",
      "Epoch 00082 | Loss 0.0554 | Train Acc 0.9851| Test Acc 0.9636\n",
      "Epoch 00083 | Loss 0.0542 | Train Acc 0.9858| Test Acc 0.9636\n",
      "Epoch 00084 | Loss 0.0530 | Train Acc 0.9873| Test Acc 0.9636\n",
      "Epoch 00085 | Loss 0.0518 | Train Acc 0.9873| Test Acc 0.9636\n",
      "Epoch 00086 | Loss 0.0507 | Train Acc 0.9873| Test Acc 0.9636\n",
      "Epoch 00087 | Loss 0.0496 | Train Acc 0.9873| Test Acc 0.9636\n",
      "Epoch 00088 | Loss 0.0485 | Train Acc 0.9873| Test Acc 0.9682\n",
      "Epoch 00089 | Loss 0.0474 | Train Acc 0.9881| Test Acc 0.9682\n",
      "Epoch 00090 | Loss 0.0463 | Train Acc 0.9888| Test Acc 0.9682\n",
      "Epoch 00091 | Loss 0.0453 | Train Acc 0.9896| Test Acc 0.9682\n",
      "Epoch 00092 | Loss 0.0443 | Train Acc 0.9896| Test Acc 0.9682\n",
      "Epoch 00093 | Loss 0.0433 | Train Acc 0.9903| Test Acc 0.9682\n",
      "Epoch 00094 | Loss 0.0424 | Train Acc 0.9910| Test Acc 0.9697\n",
      "Epoch 00095 | Loss 0.0414 | Train Acc 0.9910| Test Acc 0.9697\n",
      "Epoch 00096 | Loss 0.0405 | Train Acc 0.9918| Test Acc 0.9697\n",
      "Epoch 00097 | Loss 0.0396 | Train Acc 0.9918| Test Acc 0.9682\n",
      "Epoch 00098 | Loss 0.0387 | Train Acc 0.9918| Test Acc 0.9667\n",
      "Epoch 00099 | Loss 0.0379 | Train Acc 0.9925| Test Acc 0.9667\n",
      "Epoch 00100 | Loss 0.0370 | Train Acc 0.9933| Test Acc 0.9682\n",
      "Epoch 00101 | Loss 0.0362 | Train Acc 0.9933| Test Acc 0.9682\n",
      "Epoch 00102 | Loss 0.0354 | Train Acc 0.9940| Test Acc 0.9682\n",
      "Epoch 00103 | Loss 0.0346 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00104 | Loss 0.0339 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00105 | Loss 0.0331 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00106 | Loss 0.0324 | Train Acc 0.9955| Test Acc 0.9682\n",
      "Epoch 00107 | Loss 0.0317 | Train Acc 0.9963| Test Acc 0.9682\n",
      "Epoch 00108 | Loss 0.0310 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00109 | Loss 0.0303 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00110 | Loss 0.0296 | Train Acc 0.9970| Test Acc 0.9682\n",
      "Epoch 00111 | Loss 0.0290 | Train Acc 0.9978| Test Acc 0.9682\n",
      "Epoch 00112 | Loss 0.0284 | Train Acc 0.9978| Test Acc 0.9682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00113 | Loss 0.0277 | Train Acc 0.9978| Test Acc 0.9682\n",
      "Epoch 00114 | Loss 0.0271 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00115 | Loss 0.0265 | Train Acc 0.9978| Test Acc 0.9697\n",
      "Epoch 00116 | Loss 0.0260 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00117 | Loss 0.0254 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00118 | Loss 0.0248 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00119 | Loss 0.0243 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00120 | Loss 0.0238 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00121 | Loss 0.0233 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00122 | Loss 0.0228 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00123 | Loss 0.0223 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00124 | Loss 0.0218 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00125 | Loss 0.0213 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00126 | Loss 0.0209 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00127 | Loss 0.0204 | Train Acc 0.9993| Test Acc 0.9697\n",
      "Epoch 00128 | Loss 0.0200 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00129 | Loss 0.0196 | Train Acc 1.0000| Test Acc 0.9697\n",
      "Epoch 00130 | Loss 0.0192 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00131 | Loss 0.0188 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00132 | Loss 0.0184 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00133 | Loss 0.0180 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00134 | Loss 0.0177 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00135 | Loss 0.0173 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00136 | Loss 0.0169 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00137 | Loss 0.0166 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00138 | Loss 0.0163 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00139 | Loss 0.0159 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00140 | Loss 0.0156 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00141 | Loss 0.0153 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00142 | Loss 0.0150 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00143 | Loss 0.0147 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00144 | Loss 0.0144 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00145 | Loss 0.0141 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00146 | Loss 0.0139 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00147 | Loss 0.0136 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00148 | Loss 0.0133 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00149 | Loss 0.0131 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00150 | Loss 0.0128 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00151 | Loss 0.0126 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00152 | Loss 0.0123 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00153 | Loss 0.0121 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00154 | Loss 0.0119 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00155 | Loss 0.0116 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00156 | Loss 0.0114 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00157 | Loss 0.0112 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00158 | Loss 0.0110 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00159 | Loss 0.0108 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00160 | Loss 0.0106 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00161 | Loss 0.0104 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00162 | Loss 0.0102 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00163 | Loss 0.0100 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00164 | Loss 0.0098 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00165 | Loss 0.0096 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00166 | Loss 0.0095 | Train Acc 1.0000| Test Acc 0.9712\n",
      "Epoch 00167 | Loss 0.0093 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00168 | Loss 0.0091 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00169 | Loss 0.0090 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00170 | Loss 0.0088 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00171 | Loss 0.0087 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00172 | Loss 0.0085 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00173 | Loss 0.0084 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00174 | Loss 0.0082 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00175 | Loss 0.0081 | Train Acc 1.0000| Test Acc 0.9727\n",
      "Epoch 00176 | Loss 0.0080 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00177 | Loss 0.0078 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00178 | Loss 0.0077 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00179 | Loss 0.0076 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00180 | Loss 0.0074 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00181 | Loss 0.0073 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00182 | Loss 0.0072 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00183 | Loss 0.0071 | Train Acc 1.0000| Test Acc 0.9742\n",
      "Epoch 00184 | Loss 0.0070 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00185 | Loss 0.0068 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00186 | Loss 0.0067 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00187 | Loss 0.0066 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00188 | Loss 0.0065 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00189 | Loss 0.0064 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00190 | Loss 0.0063 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00191 | Loss 0.0062 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00192 | Loss 0.0061 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00193 | Loss 0.0060 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00194 | Loss 0.0059 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00195 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00196 | Loss 0.0058 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00197 | Loss 0.0057 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00198 | Loss 0.0056 | Train Acc 1.0000| Test Acc 0.9758\n",
      "Epoch 00199 | Loss 0.0055 | Train Acc 1.0000| Test Acc 0.9773\n"
     ]
    }
   ],
   "source": [
    "acc_list=[]\n",
    "acc_test_list=[]\n",
    "sens=[]\n",
    "spec=[]\n",
    "dice=[]\n",
    "prec=[]\n",
    "for i in range(10):\n",
    "    model_gsage = SAGE(in_feats=58, hid_feats=80, out_feats=2)\n",
    "    opt = th.optim.Adam(model_gsage.parameters(),lr=0.001)\n",
    "    model_gsage.train()\n",
    "    for epoch in range(200):\n",
    "        out = model_gsage(g_dgl_euclid_train, inputs_g_train)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(out, labels_graph_train)\n",
    "        # compute validation accuracy\n",
    "        acc = evaluate(model_gsage, g_dgl_euclid_train, inputs_g_train, labels_graph_train, t_mask2)\n",
    "        acc_test=evaluate(model_gsage, g_dgl_euclid_test, inputs_g_test, labels_graph_test, t_mask_test)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Train Acc {:.4f}| Test Acc {:.4f}\".format(\n",
    "                epoch, loss.item(), acc,acc_test))\n",
    "    acc_list.append(evaluate(model_gsage, g_dgl_euclid_train, inputs_g_train, labels_graph_train, t_mask2))\n",
    "    acc_test_list.append( evaluate(model_gsage, g_dgl_euclid_test, inputs_g_test, labels_graph_test, t_mask_test))\n",
    "#     out = model_gsage(g_dgl_euclid_test, inputs_g_test)\n",
    "#     prob=torch.softmax(out[t_mask_test],1).cpu().detach().numpy()\n",
    "#     prob=pd.DataFrame(prob,columns=['prob_blues','prob_rock'])\n",
    "#     y_pred=prob.prob_blues.apply(lambda x: 0 if x>0.5 else 1)\n",
    "    \n",
    "#     sens.append( recall_score(y_test, y_pred))\n",
    "#     spec.append(recall_score(1-y_test, 1-y_pred))\n",
    "#     dice.append(2*spec*sens/(spec+sens))\n",
    "#     prec.append(precision_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d184826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698484848484847"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_test_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
